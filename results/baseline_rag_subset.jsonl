{"id": 1, "question": "What is CLaRa and what problem does it solve?", "reference_answer": "CLaRa (Compressing LLM-based Retriever Architecture) is a system that compresses retrieved documents into compact memory representations to improve retrieval-augmented generation by reducing context length.", "category": "factual", "keywords": ["compression", "retrieval", "RAG", "memory"], "rag_answer": "CLaRa is a unified framework that performs retrieval and generation within a single continuous space, solving the issue of disjoint optimization between retrieval and generation in traditional RAG systems.", "status": "success"}
{"id": 2, "question": "How many memory tokens does CLaRa use per document?", "reference_answer": "CLaRa uses a configurable number of memory tokens, with experiments showing compression rates like 32x or 64x, typically resulting in 2-4 memory tokens per document.", "category": "technical", "keywords": ["memory tokens", "compression rate", "32x", "64x"], "rag_answer": "CLaRa uses 3 memory tokens per document.\nCorrect", "status": "success"}
{"id": 3, "question": "What are the three training stages of CLaRa?", "reference_answer": "Stage 1: Compression Pretraining with MSE loss, Stage 2: Compression Instruction Tuning, Stage 3: End-to-End Fine-tuning with both compression and generation.", "category": "detailed", "keywords": ["stage 1", "stage 2", "stage 3", "pretraining", "instruction tuning", "end-to-end"], "rag_answer": "CLaRa consists of three training stages: (1) Compressor pretraining, (2) Instruction tuning, and (3) End-to-end training.", "status": "success"}
{"id": 4, "question": "What loss functions are used in CLaRa Stage 1 training?", "reference_answer": "Stage 1 uses MSE loss for compression alignment between memory tokens and non-memory tokens, and optionally QA loss for semantic preservation.", "category": "technical", "keywords": ["MSE loss", "QA loss", "compression", "alignment"], "rag_answer": "The loss functions used in CLaRa Stage 1 training are the masked language modeling (MLM) loss and the next-token prediction (NTP) loss.", "status": "success"}
{"id": 5, "question": "How does CLaRa differ from standard RAG systems?", "reference_answer": "CLaRa compresses retrieved documents into compact memory representations instead of directly concatenating full document text, reducing context length while preserving information.", "category": "comparative", "keywords": ["compression", "standard RAG", "memory tokens", "context length"], "rag_answer": "CLaRa unifies retrieval and generation by training both components end-to-end in a shared continuous space, using a differentiable top-k estimator to enable gradient flow between modules. This contrasts with standard RAG systems, where retrieval and generation are trained separately, leading to disjoint optimization and representation spaces. CLaRa also employs a compressor to distill documents into high-quality compact representations, reducing context length and improving efficiency.", "status": "success"}
