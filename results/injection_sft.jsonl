{"id": 1, "question": "What is CLaRa and what problem does it solve?", "reference_answer": "CLaRa (Compressing LLM-based Retriever Architecture) is a system that compresses retrieved documents into compact memory representations to improve retrieval-augmented generation by reducing context length.", "category": "factual", "keywords": ["compression", "retrieval", "RAG", "memory"], "injection_answer": "CLaRa stands for Continuous Latent Reasoning. It solves the problem of end-to-end continuous reasoning in complex, multi-modal, and multimodal data.", "status": "success"}
{"id": 2, "question": "How many memory tokens does CLaRa use per document?", "reference_answer": "CLaRa uses a configurable number of memory tokens, with experiments showing compression rates like 32x or 64x, typically resulting in 2-4 memory tokens per document.", "category": "technical", "keywords": ["memory tokens", "compression rate", "32x", "64x"], "injection_answer": "CLaRa uses 4 memory tokens per document.", "status": "success"}
{"id": 3, "question": "What are the three training stages of CLaRa?", "reference_answer": "Stage 1: Compression Pretraining with MSE loss, Stage 2: Compression Instruction Tuning, Stage 3: End-to-End Fine-tuning with both compression and generation.", "category": "detailed", "keywords": ["stage 1", "stage 2", "stage 3", "pretraining", "instruction tuning", "end-to-end"], "injection_answer": "CLaRa uses a three-stage training model to teach the model to perform the retrieval and generation. The three stages are: 1) Pretraining, 2) Fine-tuning, and 3) Reasoning.", "status": "success"}
{"id": 4, "question": "What loss functions are used in CLaRa Stage 1 training?", "reference_answer": "Stage 1 uses MSE loss for compression alignment between memory tokens and non-memory tokens, and optionally QA loss for semantic preservation.", "category": "technical", "keywords": ["MSE loss", "QA loss", "compression", "alignment"], "injection_answer": "The loss functions used in CLaRa Stage 1 training are the gradient of the generator loss and the gradient of the discriminator loss.", "status": "success"}
{"id": 5, "question": "How does CLaRa differ from standard RAG systems?", "reference_answer": "CLaRa compresses retrieved documents into compact memory representations instead of directly concatenating full document text, reducing context length while preserving information.", "category": "comparative", "keywords": ["compression", "standard RAG", "memory tokens", "context length"], "injection_answer": "CLaRa uses a retrieval and generation framework to create language and instruction-tuned data, which standard RAG systems do not.", "status": "success"}
{"id": 6, "question": "What is the compression rate used in CLaRa experiments?", "reference_answer": "CLaRa experiments use compression rates of 32x and 64x, meaning documents are compressed to 1/32 or 1/64 of their original length.", "category": "technical", "keywords": ["compression rate", "32x", "64x"], "injection_answer": "The compression rate used in CLaRa experiments is 10%.", "status": "success"}
{"id": 7, "question": "What datasets were used to evaluate CLaRa?", "reference_answer": "CLaRa was evaluated on multiple QA datasets including Natural Questions, TriviaQA, HotpotQA, and other multi-hop reasoning benchmarks.", "category": "factual", "keywords": ["Natural Questions", "TriviaQA", "HotpotQA", "datasets"], "injection_answer": "The CLaRa framework was evaluated using the CLaRa datasets, which include four different datasets: CLaRa, CLaRa-mini, CLaRa-mini-mini, and CLaRa-mini-mini-mini.", "status": "success"}
{"id": 8, "question": "What is the role of memory tokens in CLaRa?", "reference_answer": "Memory tokens serve as compressed representations of documents, capturing key information while reducing context length for efficient retrieval-augmented generation.", "category": "detailed", "keywords": ["memory tokens", "compressed representation", "key information"], "injection_answer": "The role of memory tokens in CLaRa is to represent the different semantic and pragmatic information that is used to generate the responses. The memory tokens are initialized with the information from the initialization stage and are used to store the different semantic and pragmatic information that is used to generate the responses.", "status": "success"}
{"id": 9, "question": "What base model does CLaRa build upon?", "reference_answer": "CLaRa is built on top of LLaMA-based models or similar decoder-only transformers, using them as the compression and generation backbone.", "category": "factual", "keywords": ["LLaMA", "decoder", "transformer", "base model"], "injection_answer": "CLaRa builds upon the CLaRa-mini model.", "status": "success"}
{"id": 10, "question": "How does CLaRa handle multi-hop questions?", "reference_answer": "CLaRa compresses multiple retrieved documents and allows the model to reason across compressed representations to answer multi-hop questions.", "category": "detailed", "keywords": ["multi-hop", "multiple documents", "reasoning"], "injection_answer": "CLaRa uses a reranker to rerank the top QA outputs from the generator and the retriever. It then uses a reranker to rerank the top QA outputs from the generator and the retriever. It then uses a reranker to rerank the top QA outputs from the generator and the retriever. It then uses a reranker to rerank the top QA outputs from the generator and the retriever. It then uses a reranker to rerank the top QA outputs from the generator and the retriever. It then uses a reranker to rerank the top QA outputs from the generator and the retriever. It then uses a reranker to rerank the top QA outputs from the generator and the retriever. It then uses a reranker to rerank the top QA outputs from the generator and the retriever. It then uses a reranker to rerank the top QA outputs from the generator and the retriever. It then uses a reranker to rerank the top QA outputs from the generator and the retriever. It then uses a reranker to rerank the top QA outputs from the generator and the retriever. It then uses a reranker to rerank the top QA", "status": "success"}
{"id": 11, "question": "What is the purpose of the compressor in CLaRa?", "reference_answer": "The compressor transforms long documents into compact memory token representations while preserving semantic information needed for question answering.", "category": "technical", "keywords": ["compressor", "transformation", "semantic information"], "injection_answer": "The compressor is used to compress the CLaRa model and send it to the differentiable renderer for synthesis.", "status": "success"}
{"id": 12, "question": "What attention mechanism does CLaRa use?", "reference_answer": "CLaRa uses standard transformer attention mechanisms, optionally with Flash Attention 2 for efficiency.", "category": "technical", "keywords": ["attention", "transformer", "Flash Attention"], "injection_answer": "CLaRa uses a reranker to rerank the top reranker candidates.", "status": "success"}
{"id": 13, "question": "How is the compression quality measured in CLaRa?", "reference_answer": "Compression quality is measured through MSE loss between memory and non-memory token representations, and through downstream QA performance.", "category": "technical", "keywords": ["MSE loss", "quality", "QA performance"], "injection_answer": "The quality of the compression is measured by the signal-to-noise ratio (SNR).", "status": "success"}
{"id": 14, "question": "What is the doc_max_length parameter in CLaRa?", "reference_answer": "doc_max_length defines the maximum length of input documents before compression, typically set to values like 128 or 256 tokens.", "category": "technical", "keywords": ["doc_max_length", "input length", "128", "256"], "injection_answer": "The doc_max_length parameter in CLaRa is the maximum length of the document to be processed. It is used to limit the size of the input document and prevent the model from processing documents that are too long.", "status": "success"}
{"id": 15, "question": "Does CLaRa use LoRA for training?", "reference_answer": "Yes, CLaRa can use LoRA (Low-Rank Adaptation) for efficient fine-tuning of the model, particularly for the decoder and compressor components.", "category": "technical", "keywords": ["LoRA", "fine-tuning", "efficient"], "injection_answer": "CLaRa uses LoRA for fine-tuning, not for training.", "status": "success"}
{"id": 16, "question": "What optimizer is used for CLaRa training?", "reference_answer": "CLaRa uses AdamW optimizer for training, with learning rates typically around 1e-4 for pretraining and 5e-6 for end-to-end fine-tuning.", "category": "technical", "keywords": ["AdamW", "optimizer", "learning rate"], "injection_answer": "The optimizer used for CLaRa training is the Adam optimizer with a learning rate of 0.001.", "status": "success"}
{"id": 17, "question": "How does CLaRa handle document retrieval?", "reference_answer": "CLaRa uses external retrieval systems to fetch relevant documents, then compresses them using trained memory tokens for efficient processing.", "category": "detailed", "keywords": ["retrieval", "external system", "compression"], "injection_answer": "CLaRa uses a retrieval model to fetch documents from the database. It employs a combination of keyword matching and relevance scoring to ensure that the documents retrieved are pertinent to the user's query.", "status": "success"}
{"id": 18, "question": "What is the advantage of compression in CLaRa?", "reference_answer": "Compression reduces context length, enabling processing of more documents within GPU memory limits while maintaining retrieval performance.", "category": "comparative", "keywords": ["compression advantage", "context length", "GPU memory", "performance"], "injection_answer": "It allows the system to be initialized with a smaller model, which can lead to faster initialization and lower memory requirements.", "status": "success"}
{"id": 19, "question": "What is the training batch size used in CLaRa?", "reference_answer": "CLaRa uses varying batch sizes depending on the stage, with micro batch sizes for gradient accumulation and distributed training across GPUs.", "category": "technical", "keywords": ["batch size", "gradient accumulation", "distributed"], "injection_answer": "32", "status": "success"}
{"id": 20, "question": "How does CLaRa preserve semantic information during compression?", "reference_answer": "CLaRa preserves semantics through MSE loss alignment and QA loss, ensuring compressed representations retain information needed for answering questions.", "category": "detailed", "keywords": ["semantic preservation", "MSE loss", "QA loss", "information retention"], "injection_answer": "CLaRa uses reranking to retain semantic information during the compression of the data.", "status": "success"}
