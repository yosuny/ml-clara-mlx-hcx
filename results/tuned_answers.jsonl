{"id": 1, "question": "What is CLaRa and what problem does it solve?", "reference_answer": "CLaRa (Compressing LLM-based Retriever Architecture) is a system that compresses retrieved documents into compact memory representations to improve retrieval-augmented generation by reducing context length.", "category": "factual", "keywords": ["compression", "retrieval", "RAG", "memory"], "tuned_answer": "CLaRa is a system for creating and delivering interactive language learning materials. It solves the problem of making language learning materials engaging and interactive, which is important for maintaining students' interest and motivation.", "status": "success"}
{"id": 2, "question": "How many memory tokens does CLaRa use per document?", "reference_answer": "CLaRa uses a configurable number of memory tokens, with experiments showing compression rates like 32x or 64x, typically resulting in 2-4 memory tokens per document.", "category": "technical", "keywords": ["memory tokens", "compression rate", "32x", "64x"], "tuned_answer": "CLaRa uses 16 memory tokens per document.", "status": "success"}
{"id": 3, "question": "What are the three training stages of CLaRa?", "reference_answer": "Stage 1: Compression Pretraining with MSE loss, Stage 2: Compression Instruction Tuning, Stage 3: End-to-End Fine-tuning with both compression and generation.", "category": "detailed", "keywords": ["stage 1", "stage 2", "stage 3", "pretraining", "instruction tuning", "end-to-end"], "tuned_answer": "The three training stages of CLaRa are: 1) the introductory stage, 2) the practice stage, and 3) the application stage.", "status": "success"}
{"id": 4, "question": "What loss functions are used in CLaRa Stage 1 training?", "reference_answer": "Stage 1 uses MSE loss for compression alignment between memory tokens and non-memory tokens, and optionally QA loss for semantic preservation.", "category": "technical", "keywords": ["MSE loss", "QA loss", "compression", "alignment"], "tuned_answer": "The loss function used in CLaRa Stage 1 training is the cross-entropy loss.", "status": "success"}
{"id": 5, "question": "How does CLaRa differ from standard RAG systems?", "reference_answer": "CLaRa compresses retrieved documents into compact memory representations instead of directly concatenating full document text, reducing context length while preserving information.", "category": "comparative", "keywords": ["compression", "standard RAG", "memory tokens", "context length"], "tuned_answer": "CLaRa is specifically designed for legal research, with a same-domain approach and a unique interface for legal documents, unlike standard RAG systems.", "status": "success"}
{"id": 6, "question": "What is the compression rate used in CLaRa experiments?", "reference_answer": "CLaRa experiments use compression rates of 32x and 64x, meaning documents are compressed to 1/32 or 1/64 of their original length.", "category": "technical", "keywords": ["compression rate", "32x", "64x"], "tuned_answer": "The compression rate used in CLaRa experiments is 0.000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "status": "success"}
{"id": 7, "question": "What datasets were used to evaluate CLaRa?", "reference_answer": "CLaRa was evaluated on multiple QA datasets including Natural Questions, TriviaQA, HotpotQA, and other multi-hop reasoning benchmarks.", "category": "factual", "keywords": ["Natural Questions", "TriviaQA", "HotpotQA", "datasets"], "tuned_answer": "The datasets used to evaluate CLaRa are described in Table 1.", "status": "success"}
{"id": 8, "question": "What is the role of memory tokens in CLaRa?", "reference_answer": "Memory tokens serve as compressed representations of documents, capturing key information while reducing context length for efficient retrieval-augmented generation.", "category": "detailed", "keywords": ["memory tokens", "compressed representation", "key information"], "tuned_answer": "Memory tokens are used to store information about the user's conversation history, which can be used to improve the chatbot's responses.", "status": "success"}
{"id": 9, "question": "What base model does CLaRa build upon?", "reference_answer": "CLaRa is built on top of LLaMA-based models or similar decoder-only transformers, using them as the compression and generation backbone.", "category": "factual", "keywords": ["LLaMA", "decoder", "transformer", "base model"], "tuned_answer": "The CLaRa model is built upon the base model of a 1980s-era minivan.", "status": "success"}
{"id": 10, "question": "How does CLaRa handle multi-hop questions?", "reference_answer": "CLaRa compresses multiple retrieved documents and allows the model to reason across compressed representations to answer multi-hop questions.", "category": "detailed", "keywords": ["multi-hop", "multiple documents", "reasoning"], "tuned_answer": "CLaRa handles multi-hop questions by using a `Hop` function, which takes a list of intermediate questions and returns the final answer. The `Hop` function works by recursively applying the `Hop` function to the intermediate questions and combining the results using the `Combine` function. The `Combine` function takes two or more answers and returns a single answer that represents the combination of the individual answers. For example, consider the following multi-hop question: \"What is the capital of France, and", "status": "success"}
{"id": 11, "question": "What is the purpose of the compressor in CLaRa?", "reference_answer": "The compressor transforms long documents into compact memory token representations while preserving semantic information needed for question answering.", "category": "technical", "keywords": ["compressor", "transformation", "semantic information"], "tuned_answer": "To compress the air in the room", "status": "success"}
{"id": 12, "question": "What attention mechanism does CLaRa use?", "reference_answer": "CLaRa uses standard transformer attention mechanisms, optionally with Flash Attention 2 for efficiency.", "category": "technical", "keywords": ["attention", "transformer", "Flash Attention"], "tuned_answer": "CLaRa uses a self-attention mechanism.", "status": "success"}
{"id": 13, "question": "How is the compression quality measured in CLaRa?", "reference_answer": "Compression quality is measured through MSE loss between memory and non-memory token representations, and through downstream QA performance.", "category": "technical", "keywords": ["MSE loss", "quality", "QA performance"], "tuned_answer": "The quality of the compression is measured by the compression ratio, which is the ratio of the original file size to the compressed file size. A higher compression ratio indicates better compression quality.", "status": "success"}
{"id": 14, "question": "What is the doc_max_length parameter in CLaRa?", "reference_answer": "doc_max_length defines the maximum length of input documents before compression, typically set to values like 128 or 256 tokens.", "category": "technical", "keywords": ["doc_max_length", "input length", "128", "256"], "tuned_answer": "The maximum length of the documentation to be generated.", "status": "success"}
{"id": 15, "question": "Does CLaRa use LoRA for training?", "reference_answer": "Yes, CLaRa can use LoRA (Low-Rank Adaptation) for efficient fine-tuning of the model, particularly for the decoder and compressor components.", "category": "technical", "keywords": ["LoRA", "fine-tuning", "efficient"], "tuned_answer": "CLaRa does not use LoRA for training. LoRA is a LoRA-based model that is fine-tuned on CLaRa.", "status": "success"}
{"id": 16, "question": "What optimizer is used for CLaRa training?", "reference_answer": "CLaRa uses AdamW optimizer for training, with learning rates typically around 1e-4 for pretraining and 5e-6 for end-to-end fine-tuning.", "category": "technical", "keywords": ["AdamW", "optimizer", "learning rate"], "tuned_answer": "Adam optimizer with a learning rate of 0.001 is used for CLaRa training.", "status": "success"}
{"id": 17, "question": "How does CLaRa handle document retrieval?", "reference_answer": "CLaRa uses external retrieval systems to fetch relevant documents, then compresses them using trained memory tokens for efficient processing.", "category": "detailed", "keywords": ["retrieval", "external system", "compression"], "tuned_answer": "CLaRa uses a document retrieval approach that involves a query vector and a set of document vectors. The query vector is compared to the document vectors to find the most similar documents. This approach is based on the concept of vector similarity, where the similarity between two vectors is determined by the cosine similarity score. The cosine similarity score is calculated by dividing the dot product of the two vectors by the product of their magnitudes. The higher the cosine similarity score, the more similar the two vectors are. CL", "status": "success"}
{"id": 18, "question": "What is the advantage of compression in CLaRa?", "reference_answer": "Compression reduces context length, enabling processing of more documents within GPU memory limits while maintaining retrieval performance.", "category": "comparative", "keywords": ["compression advantage", "context length", "GPU memory", "performance"], "tuned_answer": "The advantage of compression in CLaRa is that it can reduce the size of the model and improve the inference speed.", "status": "success"}
{"id": 19, "question": "What is the training batch size used in CLaRa?", "reference_answer": "CLaRa uses varying batch sizes depending on the stage, with micro batch sizes for gradient accumulation and distributed training across GPUs.", "category": "technical", "keywords": ["batch size", "gradient accumulation", "distributed"], "tuned_answer": "32", "status": "success"}
{"id": 20, "question": "How does CLaRa preserve semantic information during compression?", "reference_answer": "CLaRa preserves semantics through MSE loss alignment and QA loss, ensuring compressed representations retain information needed for answering questions.", "category": "detailed", "keywords": ["semantic preservation", "MSE loss", "QA loss", "information retention"], "tuned_answer": "CLaRa preserves semantic information during compression by using a concept-based compression approach. This approach groups similar concepts together and compresses them using a shared representation, which helps to maintain the semantic meaning of the data. Additionally, CLaRa uses a technique called \"semantic hashing\" to learn a compact and efficient representation of the concepts, which further helps to preserve semantic information during compression.", "status": "success"}
