{"text": "CLaRa: Bridging Retrieval and Generation\nwith Continuous Latent Reasoning\nJie He1,2, Richard He Bai1, Sinead Williamson1, Jeff Z. Pan2, Navdeep Jaitly1‚Ä†, Yizhe Zhang1\n1Apple 2University of Edinburgh\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers\nfrom long contexts and disjoint retrieval‚Äìgeneration optimization. In this work, we propose CLaRa (Continuous Latent\nReasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous\nspace. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis\nframework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single\nlanguage modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically,\nthis unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show\nthat CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned\nbaselines.\nCode:https://github.com/apple/ml-clara\nCorrespondence:Jie He: j.he@ed.ac.uk; Yizhe Zhang: yizhe_zhang@apple.com\nDate:November 27, 2025\n1 Introduction\nRetrieval-Augmented Generation(RAG) has become a powerful paradigm for enhancing large language models\n(LLMs) across diverse NLP tasks (Lewis et al., 2020; Gao et al., 2024; Li et al., 2024b; Wu et al., 2024;\nAbootorabi et al., 2025). By incorporating external evidence, RAG mitigates key weaknesses of LLMs such as\nhallucination (Ayala & Bechard, 2024) and knowledge obsolescence (Lau et al., 2025).\nMost RAG systems suffer from a fundamental structural issue:retrieval and generation are optimized\nseparately. Retrievers select documents based on surface-level similarity, while generators produce answers\nwithout providing feedback about what information is truly needed (Shi et al., 2025). This disjoint design\nleads to two intertwined challenges.(1) Optimization.Because document selection is discrete, gradients\ncannot flow from the generator back to the retriever (Sachan et al., 2021; Lin et al., 2024), hindering joint\ntraining and preventing the retriever from aligning with the generator‚Äôs task objective.(2) Efficiency.Dense\nretrievers rank documents in embedding space, whereas generators still consume raw text, resulting in an\narchitectural mismatch. This mismatch yields (i) inconsistent representation spaces that prevent end-to-end\noptimization, (ii) redundant text processing that increases inference cost (Merth et al., 2024) and causes\ncontext overflow (Leng et al., 2024; Yue et al., 2025), and (iii) duplicated encoding for both retrieval and\ngeneration. Even if gradients could flow jointly, these inefficiencies would persist due to the lack of a shared\nlatent space.\nOur Key Insight: Shared Continuous Representations.Towards tackling this issue, we propose a unified framework\nthat performs retrieval and generation overshared continuous document representationsas shown in\nFig 1. Instead of maintaining separate embeddings and raw text, we encode documents once into compact\n‚Ä†Work done at Apple.\n1\narXiv:2511.18659v2  [cs.CL]  25 Nov 2025\nCompressor\nGeneratorQuery \nreasoner\nQuery\n Answer\nRaw Docs Compressed Docs\nRetrieved\nCompressed Docs\nSalient info\nGenerator\nCompressor Pre-Training Stage\nBackward Forward \nQuery: \nWhich \ncity is the \nliving \nplace of \nthe \ndirector \nof the \nromantic \ncomedy \n‚ÄòBig Stone \nGap‚Äô?\nWord \nEmbedding \nMatrixLook up\nQuery \nreasoner\nùêê\nCLaRa End-to-end Training Stage CLaRa End-to-end Inference Stage\nQuery embedding\nRetrieve\nDoc embeddings\ncity, big, Stone, \nciudad, Based, \nromantic, Rom, \nVirginia, charming, \nlov, Kate, sto, vare, \ndirector, film, ri, \nana, location, village, \nresidence, lives, \nmovie\nAnswer:\nNew \nYork City\nRelated documents: Adriana Trigiani is an Italian American best-selling author, television \nwriter, and film director based in Greenwich Village, New York City.\nBig Stone Gap is a 2014 American drama romantic comedy\nfilm written and directed by Adriana Trigiani. The story is set in the actual Virginia town of \nBig Stone Gap circa 1970s.\nAdriana Trigiani is an Italian \nAmerican best-selling author, \ntelevision writer, and film \ndirector based in Greenwich \nVillage, New York City.\nBig Stone Gapis a 2014 \nAmerican drama romantic \ncomedy film directed by \nAdriana Trigiani.The story is \nset in the actual Virginia town \nof Big Stone Gap circa 1970s.\nContinuous \ntokens\nFigure 1(a) During training, we firstpretrain the compressorto encourage it to retain only essential information.\nNext, weperform offline compressionof the documents. After that, we encode the query using thequery reasoner,\nretrieve the compressed document representations for generation, anduse only the final next-token prediction\nlossto jointly update both the query reasoner and the generator. (b) An example from the inference stage: the\ntokens represent key clue words related to the question. When we decode the continuous query embedding, we find\nthat it contains information not present in the original query, indicating that it has learned some of the intermediate\nreasoning keywords.\nmemory-token representations that serve both purposes. A central motivation behind this design is that\nsupervised retrieval training typically relies on relevance-labeled data, which is scarce and often domain-specific.\nTo overcome this limitation, we propagate the next-token prediction (NTP) loss from the generator to the\nretriever, providing a weakly supervised signal that naturally adapts retrieval to downstream generation\nobjectives. This mechanism allows the retriever to learn which documents truly enhance answer generation\nrather than relying on surface-level similarity. Moreover, continuous representations and joint optimization\nare inherently complementary: continuous encodings make the retrieval process differentiable, while joint\ntraining aligns both modules within a shared semantic space optimized for reasoning.\nThis unified design resolves both challenges simultaneously.Optimization-wise, continuous representations\nenable differentiable top-k selection via Straight-Through estimation, allowing generator gradients to update\nthe retriever directly through gradient descent rather than inefficient RL sampling.Efficiency-wise, shared\nencodings remove redundant computation and drastically reduce context length, enabling fully end-to-end\noptimization and inference within the same representation space.\nTo realize this vision, we presentCLaRa(Continuous Latent Reasoning), a joint retrieval‚Äìgeneration\nframework built on shared compressed representations. In Stage I, we proposeSCP(Salient Compressor\nPretraining), which enhances semantic fidelity by constructing QA pairs that emphasize salient document\ncontent beyond surface reconstruction. In Stage II, CLaRa performsend-to-end joint trainingof the\nquery encoder and answer generator under a unified next-token prediction loss, withdifferentiable top-k\nselectionvia Straight-Through estimation. Theoretically, we show this unified objective yields valid gradients\nfor retriever learning without explicit labels.\nWe evaluate CLaRa on four single-hop and multi-hop QA benchmarks withMistral-7BandPhi-4B. Results\nshow that SCP produces semantically rich compressed representations, and CLaRa achieves state-of-the-art\nretrieval and generation performance‚Äîoutperforming both supervised and unsupervised baselines, and even\nsurpassing full-text fine-tuned models on several tasks.\n2 SCP: Salient Compressor Pretraining\nPrevious methods (Louis et al., 2025a; Cheng et al., 2025) typically use token-level reconstruction loss to learn\ndoc representation. However, the learned representation may waste limited capacity/budget on token-by-token\nreconstruction which might be trivial. Also, the raw representation learned in such a way might not ‚Äúdigest‚Äù\nthe document exhaustively. To enable the model to focus on abstracting and digesting semantically informative\nrepresentations, we first synthesize pre-training data that highlights salient information. Based on this data,\nwe train a compression framework, where a compressor learns to retain merely essential semantics (Figure 2).\n2\nFigure 2Overview of theSCP(Salient Compressor Pretraining) framework. It includes (a) synthetic data construction\nfor pretraining, (2) compressor training using the pretraining data.\n2.1 Guided Data Synthesis for Semantic Preservation\nWe first construct a synthetic dataset where salient information is explicitly exposed through QA and\nparaphrasing. This way, the compressor later learns to identify, digest and retain thesemantic coreof the\ntext by deeply processing the raw token-level information. As shown in Figure 2 (a), our synthetic data\ngeneration pipeline consists several steps: (1) salient information elicitation via QA and paraphrase generation,\n(2) automatic verification of coverage, and (3) regeneration of missing content.\nSalient Information Elicitation.Using 2M sampled Wikipedia-2021 documents (Izacard et al., 2023), a locally\ndeployed LLM (Qwen-32B) generates three complementary forms of supervision:\n‚Ä¢Simple QA:each pair captures a single fact, encouraging fine-grained factual retention. To avoid\nredundancy, the model is guided to extract distinct atomic facts that have not been covered by previous\nquestions. This helps ensure broad coverage of salient information while keeping each question focused.\nExample: Q: ‚ÄúIn which plant family isTrichocladus crinitusclassified?‚Äù A: ‚ÄúTrichocladus crinitus\nbelongs to the plant family Hamamelidaceae (the witch-hazel family).‚Äù\n‚Ä¢Complex QA:each pair integrates multiple facts, promoting relational reasoning and higher-level\nabstraction. During generation, the model is encouraged to connect previously unlinked facts, which\nreduces repetition and increases coverage of relational information.Example: Q: ‚ÄúWhich player joined\nNewport County on 2 October 2014 and made his debut as a second-half substitute in a 1‚Äì0 defeat at\nOxford United on 4 October?‚Äù A: ‚ÄúThe player is James Loveridge.‚Äù\n‚Ä¢Paraphrase:paraphrased documents reorder sentences structure, thus altering the surface form of the\ntext while preserving its core semantics. Learning such a mapping from original text to paraphrased text\nthrough an information bottleneck of continuous representation will enable the learned representation\nto focus on the semantics.\nQA pairs distill fact-centric supervision as they tell the model which details are essential for answering\nmeaningful questions. Paraphrases, in contrast, demonstrate expression, level compactness, how to rephrase\nthe same content more efficiently. Together, they form complementary signals: factual grounding and linguistic\ncompactness.\nVerification and Regeneration.Each document and its generated outputs (QA pairs or paraphrases) are verified\nby the same LLM for factual consistency and information coverage. When missing information is detected,\n3\nthe LLM reviews both the original text and existing QA pairs to generate additional ones capturing uncovered\nfacts, iteratively up to ten rounds. Samples failing final coverage criteria are excluded. This iterative check\nensures the model only learns from fully covered, factually faithful pairs.\n2.2 Compressor Pretraining\nFollowingPISCO(Louis et al., 2025a), we adopt a shared base model equipped with multipleLoRAadapters\nfor modular control, where each adapter corresponds to a distinct function (compression or generation) as\nillustrated in Figure 2 (b).\nCompression and Generation.Given a document di = {t1, . . . , tm}, we append l learnablememory tokens\n(m1, . . . , ml)and activate only thecompressor LoRA Œ∏c. The final-layer hidden states of the memory tokens\nform the compressed representation:\nMi = LLM Œ∏c ([t1, . . . , tm, m1, . . . , ml])[m+1 :m+l].\nThe compressed vectorMi is concatenated with an instructionI to form T = [I; Mi]. During pretraining, I\ncorresponds to general text-generation tasks (e.g., QA or paraphrasing), and later is replaced with task-specific\nprompts during instruction tuning. Only thegenerator LoRAŒ∏g is trained via cross-entropy loss:\nLCE(Œ∏c, Œ∏g) =‚àí\nX\n(di,I,R ‚àó\ni )\n|R‚àó\ni |X\nt=1\nlogp Œ∏g\n\u0000\na‚àó\ni,t |I, M i, a‚àó\ni,<t\n\u0001\n.(2.1)\nCompression Alignment.To ensure that the compressed representation faithfully reflects the semantics of the\noriginal document, we encourage their latent representations to remain aligned. Intuitively, the memory tokens\nshould summarize the same semantic space as the document tokens, rather than drifting to unrelated regions.\nTherefore, we minimize the mean squared error (MSE) between the averaged hidden states of document\ntokens and memory tokens:\nLMSE =\n\r\r\r\r\r\r\n1\n|di|\nX\nt‚ààdi\nht ‚àí 1\nl\nlX\nj=1\nhmj\n\r\r\r\r\r\r\n2\n2\n.(2.2)\nThe total training loss is:\nLtotal =L CE +ŒªL MSE,(2.3)\nwhereŒªbalances semantic alignment and generative quality.\nInstruction Tuning.The pretrained compressor is for general purpose. To adapt the compressor for downstream\nQA and also obtain an answer generator that can comprehend the continuous document representation, we\noptionally performed an additional instruction finetuning training. Note that this step is not necessary. It is\nfor evaluation and ablation purpose, and we show the comparison in our experiments.\nTo achieve this, we use downstream training datasets in which the retrieved documents are paired with task\ninstructions. These document‚Äìinstruction pairs form the input to the model, while the output is a reference\nresponse generated by a teacher model conditioned on the same retrieved documents and instructions. Similar\nto the compressor pretraining stage, we jointly finetune the LoRA adapters of both the compressor and the\nanswer generator during this instruction-tuning process.\n3 CLaRa: Retrieval and generation joint training\nWhile the compressor distills documents into compact representations, a key challenge is how to retrieve and\nleverage these representations effectively for downstream reasoning. Conventional RAG systems train the\nretriever and generator separately, often resulting in a mismatch between what retrieval considers ‚Äúrelevant‚Äù\nand what generation truly needs, as well as two isolated representation spaces. To address this, we propose\n4\nFigure 3CLaRa end-to-end training: update query reasoner (Œ∏qr) and generator (Œ∏g) via language modeling loss using\ncandidate document‚Äìquestion‚Äìanswer triples.\nCLaRa, which unifies retrieval and generation by training both within a single pretrained LLM through a\ndifferentiable retrieval module. Achieving end-to-end training, however, requires a retrieval space that is\nboth stable and computationally manageable‚Äîfull documents are far too large to be re-indexed throughout\noptimization. To solve this, we use the compressor trained in SCP to producehigh-qualitycompact\nrepresentations that remain stable even when the rest of the model updates. By retrieving over these frozen\ncompressed vectors, CLaRa can support end-to-end optimization of retrieval and generation using only a\nshared cross-entropy loss, without requiring relevance-labeled data.\nFramework OverviewAs shown in Figure 3, each of the document is compressed into a dense embedding\nMi = Œ∏c(ti)using a pretrained compressor Œ∏c. The compressor remains frozen to allow offline document\nencoding. We then train a query reasoner (Œ∏qr), a LoRA adapter initialized fromŒ∏c, to represent queries in the\nsame space and with the same number of memory tokens as document representations. Through next-token\nprediction (NTP) training,Œ∏qr learns not only to encode query intent but also to anticipate relevant document\ncontent, enhancing retrieval and answer generation. For example, given ‚ÄúWhich city hosted the first modern\nOlympic Games?‚Äù, an embedding-based retriever may miss ‚Äúfirst‚Äù or ‚Äúmodern,‚Äù whereas the NTP-trained\nquery reasoner favors documents mentioning ‚ÄúAthens 1896,‚Äù which better satisfies retrieval with reasoning\nneeds. We use cosine similarity to determine the relevance between query and documents:\nsi = cos(q,M i), i= 1, . . . , D.(3.1)\nThe top-k document compressed embeddings{M1, . . . ,Mk} are concatenated withQ and fed to the generator\n(activated by Œ∏g), which produces the final answer. During training, bothŒ∏qr and Œ∏g are updated via the\nunified language modeling loss:\nLCLaRa(Œ∏q, Œ∏g) =‚àí\n|R‚àó|X\nt=1\nlogp Œ∏g\n\u0000\na‚àó\nt\n\f\f Q,M (1:k), a‚àó\n<t\n\u0001\n,(3.2)\nwhere R‚àó = ( a‚àó\n1, . . . , a‚àó\n|R‚àó|)denotes the reference output. Importantly, this allows the retriever (implicitly\nrepresented byŒ∏qr) tolearn through weak supervision from the generation objective, without explicit\nreranking labels. Finding real supervised data might be challenging, and our method is data free as it relies\nonly on downstream next token prediction objective to reason on how to retrieve the doc that maximize the\nlikelihood of downstream generation, thus is more flexible and adaptive.\nDifferentiable Top-k SelectionIn the CLaRa framework, retrieval and generation are trained jointly, yet\ntheir connection is mediated by the top-k selection of relevant documents. However, this discrete operation\nintroduces abroken gradientproblem: the generator‚Äôs supervision cannot propagate back to inform the\nretriever why certain documents should be preferred over others. To address this issue, we introduce top\nK selector via Straight-Through (ST) estimator, which conceptually acts as a ‚Äúsoft lens‚Äù ‚Äî preserving the\ndiscrete retrieval behavior during inference while allowing smooth gradient feedback during training (see\nAlgorithm 1 in Appendix for details).\n5\nGiven cosine similaritiess= [s 1, . . . , sD], temperatureœÑ, and masking for previously selected items, the soft\nand hard selections are defined as:\nZsoft[b, j,:] = softmax\n\u0012 sb + log(maskb +Œµ)\nœÑ\n\u0013\n,(3.3)\nZhard[b, j, i] =\n(\n1,ifi= arg max i‚Ä≤ Zsoft[b, j, i‚Ä≤],\n0,otherwise, (3.4)\nand the final objective that combines the hard and soft representations through a ST estimator is:\nZ=Z hard +\n\u0000\nZsoft ‚àíSG(Z soft)\n\u0001\n,(3.5)\nwhere SG(¬∑)denotes the stop-gradient operator. This maintains discrete behavior in the forward pass while\nenabling differentiable training throughZsoft. The aggregated top-k document representation is then computed\nas:\nM(k) =ZM,(3.6)\nwhere M‚ààR B√óD√ód is the matrix of all candidate embeddings with B denoting the batch size, D the number\nof candidate documents, and d the dimensionality of each document representation, typically defined as the\nproduct of the number of memory tokens and the hidden dimension of the underlying LLM.\nTheoretical Justification: Gradient Coupling AnalysisWe provide a theoretical justification for why learning from\nNTP may yield stronger and more stable learning signals for theŒ∏qr.Note that Œ∏qr can also be pre-trained\nindependently using contrastive learning over positive and negative document‚Äìquery pairs(d+, q)and( d‚àí, q),\nfollowing the DPR framework(Zhou & Chen, 2025).For query x and document d, the retrieval scoresxd\ndefines\np(d|x) = exp(sxd/œÑ)P\nj exp(sxj/œÑ) , p(y|x) =\nX\nd\np(d|x)p(y|x, d),L=‚àílogp(y|x).\nWhen reranking and generation share same representations,p(y|x, d)depends on sxd, allowing generator\ngradients to flow into the reranker. With shared embeddingsr =P\nd œÄd(s)zd and a Straight-Through estimator\nfor top-k, the gradient is\n‚àÇL\n‚àÇsxd\n=‚àí 1\np(y|x)\n\u0014\np(d|x)\n\u0000\np(y|x, d)‚àíp(y|x)\n\u0001\n+ p(y|x, r)\nœÑ p(d|x)g‚ä§(zd ‚àír)\n\u0015\n,\nwhereg=‚àá r logp(y|x, r).\nInterpretation:Intuitively, the gradient coupling allows the retriever to receive two complementary\nlearning signals. First, it is rewarded for ranking the correct documents higher through probabilistic\nalignment betweenp(d|x)and p(y|x, d). Second, it is guided to represent documents in a way that facilitates\nthe generator‚Äôs reasoning via representation-level feedback from the gradientg. Together, these dual signals\nstabilize training‚Äîrather than oscillating between strong retrieval and weak generation, both modules\nprogressively align within a shared semantic space, ensuring consistent retrieval‚Äìgeneration behavior (see\nAppendix A for details).\nCase Study: Query Reasoner Œ∏qr To probe the information embedded within the query reasonerŒ∏qr, we adopt\nthelogit lensanalysis technique (nostalgebraist, 2020). For each memory embedding, we project it through the\nLLM‚Äôs output head and record the top-50 tokens with the highest logits as topic tokens. We then aggregate\nand filter these decoded tokens to remove trivial elements such as punctuation or special symbols. As shown\nin Table 1, for the query‚ÄúHow many yards did the nephew of Ivory Lee Brown get during his 2004 true\nfreshman season?‚Äù, the query embeddings decoded from the reasoner include the token‚ÄúNFL‚Äù, ‚ÄúOklahoma‚Äù,\ndespite the fact that this word does not appear in the question itself. Interestingly, this token also occurs in\nthe corresponding positive document and serves as a crucial clue for answering the question. This finding\nindicates that our end-to-end optimization enables the query reasoner to implicitly encode reasoning-relevant\nknowledge aligned with the gold evidence, thus enhancing retrieval accuracy and semantic alignment compared\nto baseline systems.\n6\nAnalysis of Decoded T okens from Query Reasoner via Logit Lens\nQuestion:How many yards did the nephew of Ivory Lee Brown get during his 2004 true freshman season?\nReasoned topics from query representation:Truly, Nep, IV, four, yards, NFL, Oklahoma, Ned, Neil, Howard, Kentucky...\nRetrieved Documents:\n[1]...Adrian Lewis Peterson (born March 21, 1985) is an American football running back for the New Orleans Saints of the National\nFootball League (NFL). He played college football at Oklahoma and was drafted by the Minnesota Vikings seventh overall in the\n2007 NFL Draft. Peterson set the NCAA freshman rushing record with 1,925 yards as a true freshman during the 2004 season...\n[2]...Ivory Lee Brown (born August 17, 1969) is a former professional American football running back in the National Football\nLeague and World League of American Football. He played for the Phoenix Cardinals of the NFL and the San Antonio Riders of\nthe WLAF. Brown is the uncle of Minnesota Vikings running back Adrian Peterson...\n...\nAnswer: 1,925 yards\nTable 1Analysis of Decoded Tokens from Query Reasoner via Logit Lens. The highlighted tokens (red) denote the new\ninformation reasoned by the query reasoner, while (blue) denotes key evidence for solving this multihop task.\nModels CR NQ HotpotQA Musique 2Wiki A verage\nNormal\nAutocompressor 1x 17.24 14.61 3.81 19.89 13.89\nMistral-7B w/o BGE retrieval 1x 35.01 27.55 5.38 38.45 26.6\nMistral-7B w/ BGE retrieval 1x 54.58 42.94 8.94 44.24 37.67\nPhi4-mini w/o BGE retrieval 1x 18.77 21.10 4.05 30.26 18.55\nPhi4-mini w/ BGE retrieval 1x 48.14 37.78 8.11 35.11 32.28\nllmlingua-2 4x 47.53 37.05 9.02 44.35 34.49\nSCP-Mistral-7B 4x 57.05+9.5245.09+8.0410.34+1.3246.94+2.5939.86+5.37\nSCP-Phi4-mini 4x 53.31+5.7842.36+5.318.73-0.2945.22+0.8737.40+2.91\ncoconum 16x 24.12 21.48 3.52 24.48 18.40\npcc 16x 31.38 22.29 3.43 19.47 19.14\npisco 16x 54.39 41.94 10.09 44.88 37.83\nSCP-Mistral-7B 16x 55.56+1.1743.72+1.7810.55+0.4646.00+1.1238.96+1.13\nSCP-Phi4-mini 16x 51.96-2.4340.86-1.088.61-1.4844.27-0.6136.42-1.42\nxrag 128x 32.35 25.16 3.64 28.79 22.48\nSCP-Mistral-7B 128x 53.36+21.0141.37+16.2110.26+6.6246.40+17.6137.85+15.37\nSCP-Phi4-mini 128x 43.09+10.7433.92+8.766.87+3.2343.70+14.9131.90+9.42\nOracle\nAutocompressor 1x 29.47 19.24 7.16 26.74 20.65\nMistral-7B w/ BGE retrieval 1x 71.64 70.77 45.72 68.83 64.24\nPhi4-mini w/ BGE retrieval 1x 66.10 64.06 37.07 52.69 54.98\nllmlingua-2 4x 63.99 52.42 27.47 53.92 49.45\nSCP-Mistral-7B 4x 76.50+12.5173.81+21.3946.26+18.7970.48+16.5666.76+17.31\nSCP-Phi4-mini 4x 73.67+9.6872.41+19.9940.13+12.6664.22+10.3062.61+13.16\ncoconum 16x 25.61 21.72 3.64 24.63 18.90\npcc 16x 49.62 34.56 18.25 27.56 32.50\npisco 16x 73.44 66.53 33.80 60.45 58.55\nSCP-Mistral-7B 16x 75.48+2.0470.79+4.2643.15+9.3566.16+5.7163.90+5.35\nSCP-Phi4-mini 16x 73.17-0.2770.26+3.7338.39+4.5963.15+2.7061.24+2.69\nxrag 128x 42.60 30.21 7.03 30.94 27.70\nSCP-Mistral-7B 128x 69.96+27.3662.09+31.8830.86+23.8359.08+28.1455.50+27.80\nSCP-Phi4-mini 128x 60.44+17.8451.52+21.3119.28+12.2550.29+19.3545.38+17.68\nTable 2Compressor performance on four QA datasets. The best performance is highlighted in bold. We show the\nabsolute performance change (¬±) of our method under different compression rates relative to its corresponding best\nbaseline performance. CR denotes compression rate.\n4 Experiments\n4.1 Experimental setup\nDatasetsFollowing prior work Shi et al. (2025), we evaluate both the compressor and the end-to-end\nframework on the full development sets of four widely used question answering benchmarks:NQ(Kwiatkowski\net al., 2019),HotpotQA(Yang et al., 2018),MuSiQue(Trivedi et al., 2022), and2WikiMultihopQA(Ho\net al., 2020).\n7\nBaselinesFor compressor evaluation, we benchmark against both classical and recent methods, including\nAutoCompressor(Chevalier et al., 2023),XRAG(Cheng et al., 2025),COCOM(Rau et al., 2025),PCC\n(Dai et al., 2025),LLMLingual-2(Pan et al., 2024), andPISCO(Louis et al., 2025a). For reranking,\nwe compare withBM25,BGE-Reranker(Chen et al., 2023),RankZephyr-7B(Pradeep et al., 2023),\nSetwise(Zhuang et al., 2024), andRank-R1(Zhuang et al., 2025). End-to-end QA results are evaluated\nagainst representative RAG systems, including prompt-based (GenGround(Shi et al., 2024),In-Context\nRAG), retrieval-optimized (ReComp(Xu et al., 2024),DPA-RAG(Dong et al., 2025)), fine-tuned LLMs\n(Self-RAG(Asai et al., 2024),Retrobust(Yoran et al., 2024),ChatQA(Liu et al., 2025)), and jointly\noptimized models (DDR-RAG(Li et al., 2024a),DRO(Shi et al., 2025)). Unlike all baselines operating\non raw text, our method is the first to jointly optimize reranking and generation directly overcompressed\nrepresentations. Full experimental settings are provided in Appendix B. Below, we summarize the key\nfindings, while the complete set of additional experiments can be found in the Appendix, including pretraining\ndata analysis (App.C & D), training process analysis (App.E), fidelity and grounding evaluations (App.F), as\nwell as further module analyses (App.G).\n4.2 Evaluation of Compression Effectiveness\nWe evaluate ourdocument compressorunder two settings:NormalandOracle. In theNormalsetting,\nthe model retrieves the top-5 documents fromWikipedia-2021 for each query. In theOraclesetting, the\nannotated positive document is included among the top-5 to isolate compression quality from retrieval noise.\nTable 2 summarizes results across compression ratios. For full results, please refer to table 6 Our method con-\nsistently outperforms all baselines. Compared to the best soft compression modelPISCO, our model achieves\naverage gains of1.13%(Normal) and5.35%(Oracle); over the hard compression baselineLLMLingual-2,\nimprovements reach5.37%and17.31%, highlighting stronger semantic preservation.\nSurprisingly, our model exceeds the text-basedw/ BGE retrievalbaseline using uncompressed documents,\nwith average gains of2.36%on Mistral-7B and6.36%on Phi-4-min. This implies that well-trained\nsoft compression can retain essential reasoning information while substantially reducing input length. This\nmay be because the compressed representations filter out irrelevant content and focus the generator on the\nreasoning-relevant context, leading to better generalization than raw text inputs. While performance declines\nat extreme compression (beyond 32√óin Oracle), the drop remains moderate under Normal conditions due to\nweaker document relevance.\n4.3 Joint Training Results\nFor end-to-end learning, we evaluate our model under bothNormalandOraclesettings. In the Normal setup,\neach query retrieves the top-20 documents fromWikipedia-2021; the Oracle setup adds annotated positives\nto the 20-document pool to isolate generation quality from retrieval noise. We compare two initialization\nstrategies for joint reranking‚Äìgeneration training: (i) from the compression pretraining checkpoint, and (ii)\nfrom the instruction-tuned compressor. Results are shown in Table 3, with full results in Table 7 in Appendix.\nUnder the Normal setting, performance remains stable across compression ratios, peaking at16‚Äì32√ó. As\n4√ómight be harder to optim w/ NTP,CLaRa-Mistral-7Bwith 16x surpasses the text-basedDRO-\nMistral-7B, improving F1 from51.01‚Üí51.41onNQand43.65‚Üí47.18on2Wiki. In the Oracle setting,\nperformance rises notably‚ÄîF1 exceeds75%on bothNQandHotpotQA‚Äîshowing that joint optimization\neffectively exploits accurate retrieval.\nInstruction-tuned initialization outperforms pretraining-based initialization under Normal conditions, especially\non NQ and HotpotQA, indicating stronger alignment between compression and answering. However, the gap\nnarrows in the Oracle setting, suggesting initialization matters less when retrieval is reliable. Overall,CLaRa\ndemonstrates robust and scalable performance across retrieval qualities and compression ratios.\n4.4 Retrieval performance\nWe evaluate our method on thedocument rerankingtask to assess retrieval effectiveness under theOracle\nsetting, where positive documents are guaranteed in the candidate set, allowing accurate computation of\n8\nModels CR NQ HotpotQA Musique 2Wiki A verage\nF1 EM F1 EM F1 EM F1 EM F1 EM\nPrompting-based Method\nGenGround* 1x 42.31 40.60 44.71 41.27 24.36 20.77 42.58 39.61 38.49 35.56\nIn-context RAG* 1x 44.69 38.07 41.27 37.14 20.11 16.78 41.02 38.51 36.77 32.62\nRetrieval tuning\nRECOMP* 1x 42.67 37.47 42.72 38.72 24.96 17.34 38.26 32.17 37.15 31.43\nDPA-RAG* 1x 44.31 37.29 40.53 37.15 20.36 18.45 39.66 39.02 36.22 32.98\nLLM Fine-tuning\nRetRobust* 1x 43.82 37.03 40.54 35.59 18.16 18.11 39.11 38.65 35.41 32.34\nChatQA* 1x 34.54 23.64 44.60 33.40 17.05 16.64 31.90 26.80 32.02 25.12\nSelf-RAG* 1x 31.63 29.74 27.30 16.30 21.50 9.43 27.33 23.52 26.94 19.75\nEnd-to-end optimization\nDDR-RAG* 1x 28.76 40.74 35.44 31.71 10.57 13.54 38.40 35.44 28.29 30.36\nDRO-Mistral-7B* 1x 51.01 42.41 47.87 40.37 25.32 21.36 43.65 42.12 41.96 36.56\nCLaRa-Mistral-7B\n(Pretrianing-initialized)\n4x 40.62 31.21 39.53 29.54 14.53 6.16 42.59 38.49 34.32 26.35\n16x 41.75 32.24 44.37 33.72 15.36 6.99 43.47 39.50 36.24 28.11\n32x 40.68 31.36 41.84 31.26 15.32 6.66 43.23 38.98 35.27 27.06\nCLaRa-Mistral-7B\n(Instruction-initialized)\n4x 48.21 38.16 45.93 35.12 17.49 8.11 47.18 43.11 39.70 31.12\n16x 50.89 41.02 47.62 36.67 18.01 8.44 44.66 40.48 40.30 31.65\n32x 49.72 39.88 45.73 34.85 16.83 7.82 42.57 38.41 38.71 30.24\nOracle data setting\nCLaRa-Mistral-7B\n(Pretrianing-initialized)\n4x 77.80 70.52 77.66 64.83 41.59 30.33 73.20 69.14 67.56 58.70\n16x 73.81 65.74 69.57 56.76 31.15 21.18 65.90 61.31 60.11 51.25\n32x 72.03 63.65 70.91 57.07 33.40 22.22 66.32 61.12 60.66 51.02\nCLaRa-Mistral-7B\n(Instruction-initialized)\n4x 75.63 67.64 69.66 56.92 33.19 22.42 73.86 69.74 63.08 54.18\n16x 71.54 63.29 71.17 57.54 30.77 20.56 60.37 55.73 58.46 49.28\n32x 69.75 65.17 68.87 55.20 28.87 18.45 64.38 59.32 57.97 49.53\nTable 3End-to-End QA Performance. * indicates results reported from the DRO paper. CR denotes compression\nrate. The highest scores are shown inbold, and the second-best ones areunderlined. Overall, our method achieves\ncomparable performance while reducing the required context length by 16√ó.\nRecall@k.. To compare supervision levels, we introduce a fully supervised retriever baseline,Sup-Instruct,\nwhich fine-tunes theQuery Reasonervia contrastive learning with annotated positive and negative documents.\nIn contrast, our method trains the retriever in aweakly supervisedmanner, only using the next token\nprediction loss from the downstream generation.Notably, our method does not rely on any supervised\ndata of annotated document relevance labels.\nAs shown in Fig.4 (full results in Table8),CLaRa-Mistral-7Binitialized from pretraining consistently\noutperformsitsinstruction-tunedversion, indicatingthatinstructiontuning, whileimprovinganswergeneration,\nbiases the model toward localized evidence at the cost of global semantics crucial for retrieval.\nRemarkably, under the pretraining-initialized setup,CLaRaeven surpassesthe fully supervisedSup-\nInstructusing ground-truth relevance labels. OnHotpotQA(compression ratio 4), it achieves aRecall@5\nof96.21%, exceeding the strongest supervised baselineBGE-Reranker(85.93%) by+10.28%. Despite\nrelying solely on weak generation supervision,CLaRapresumably captures deep semantic correlations between\nqueries and documents and adapts to the downstream scenarios, achieving retrieval quality on par with or\nsurpassing fully supervised models.\n5 Ablation Study\nPretraining Data MixEach document in our setup is paired with two output types: (i) QA-style question‚Äìanswer\npairs and (ii) paraphrased documents. To assess the impact of data composition, we vary pretraining objectives\nand report results in Table 4 and 20. For bothMistral-7BandPhi4-mini, using eitherSimpleQAorPara-\nphrasealone already outperforms the no-pretraining baseline, showing that factual reasoning and paraphrastic\nrewriting both enrich compressed representations. Combining multiple QA types (SimpleQA+ComplexQA)\nor adding paraphrases (SimpleQA+ComplexQA+Para) achieves the best performance, confirming that di-\nverse objectives enhance semantic coverage and generalization‚Äîespecially under theOraclesetting, where\nhigh-quality retrieval amplifies pretraining benefits.\n9\nFigure 4Retrieval performance (Recall@1/3/5) on theMistral-7Bmodel across different reranking methods under\ncompression ratios = 4 and various initialization settings on NQ and HotpotQA datasets.Sup-denotes models\ntrained with labeled data using contrastive learning for the reranker.-Pretraindenotes experiments conducted using\nthe model checkpoint obtained after pretraining, while-Instructdenotes experiments conducted using the model\ncheckpoint obtained after instruction tuning.\nModels Data composition NQ HotpotQA Musique 2Wikiqa Average\nOracle\nMistral-7B\nNo-pretrain 70.01 61.13 29.00 57.43 54.39\nSimpleQA 72.66+2.6566.41+5.2835.29+6.2961.22+3.7958.90+4.51\nPara 73.86+3.8568.64+7.5136.86+7.8663.22+5.7960.64+6.25\nSimpleQA+ComplexQA 74.34+4.3369.31+8.1836.70+7.7063.71+6.2861.02+6.63\nSimpleQA+ComplexQA+Para 73.77+3.7669.51+8.3838.31+9.3164.54+7.1161.53+7.14\nPhi4-mini\nNo-pretrain 65.54 60.32 27.31 56.39 52.39\nSimpleQA 68.70+3.1664.60+4.2830.41+3.1057.46+1.0755.29+2.90\nPara 67.90+2.3664.72+4.4031.11+3.8058.67+2.2855.60+3.21\nSimpleQA+ComplexQA 69.33+3.7965.15+4.8331.15+3.8457.94+1.5555.89+3.50\nSimpleQA+ComplexQA+Para 69.90+4.3665.32+5.0031.77+4.4658.52+2.1356.38+3.99\nTable 4Effect of pretraining data composition on instruction-tuning performance under Oracle (gold context) settings\nunder the 32 compression ratio. We report the absolute score change (¬±) for each pretraining data setting relative to\nthe No-Pretrain baseline.\nEffect of MSE LossWe analyze the effect of the mean-squared error (MSE) loss in Eq.2.2, which aligns\ncompressed and original document representations. As shown in Table 5 and 21, the improvements are modest\n(0.3‚Äì0.6 points on average) but consistent across datasets, confirming that the MSE loss facilitates semantic\npreservation during compression. To provide a qualitative perspective, we visualize 4K document embeddings\nand their corresponding compressed representations using t-SNE (Figure 8 in Appendix). Without the MSE\nloss, the two distributions are clearly separated, reflecting a weak correspondence between the memory-token\nand document spaces. When the MSE loss is applied, the compressed embeddings exhibit strong overlap\nwith the original document representations, demonstrating that the alignment objective effectively enforces\nsemantic consistency between embedding spaces.\nModels CR NQ HotpotQA Musique 2Wikiqa Average\nOracle\nMistral-7B 32x 74.65 69.05 37.32 62.98 61.00\nw/ mse 32x 73.77-0.8869.51+0.4638.31+0.9964.54+1.5661.53+0.53\nMistral-7B 128x 71.24 62.26 29.29 57.87 55.16\nw/ mse 128x 69.96-1.2862.09-0.1730.86+1.5759.08+1.2155.50+0.34\nTable 5Instruction-tuning performance with and without the MSE loss under different compression ratios (CR = 32,\n128) and oracle retrieval settings.\n10\n6 Related Work\n6.1 Embedding-based/Soft Compression\nRecent studies have leveraged LLMs to compress lengthy RAG documents into continuous embeddings\nfor QA tasks (Chevalier et al., 2023; Ge et al., 2024; Mu et al., 2023; Xiao et al., 2025; Dai et al., 2025;\nKuratov et al., 2025). Generally, they shorten contexts using continuous representations but are trained\nindependently of LLMs and do not support retrieval‚Äìgeneration co-optimization. Cheng et al. (2025) propose\na projection module mapping each document to a single-vector representation while freezing encoder and\ndecoder parameters, achieving high compression but losing fine-grained semantics essential for RAG. Louis\net al. (2025a) introducePISCO, which replaces documents with variablememory-tokenrepresentations and\njointly trains the encoder and decoder for tighter coupling between compression and generation. While they\nsuggest pretraining offers limited gains with sufficient instruction data, our results show a more targeted\npretraining objective can still yield richer and more informative representations. The most related work, Louis\net al. (2025b), jointly trains a query-aware compression model that also functions as a retriever. However,\nrequiring re-compression per query contradicts the goal of reusable, query-independent representations and\nincreases latency. In contrast, our approach enables efficient, fullylabel-freeretriever learning.\n6.2 End-to-End Optimization for Retrieval and Generation\nReinforcement learning approaches(Shi et al., 2025) allow joint optimization but are unstable and\ncomputationally heavy, still relying on raw text.Differentiable reranking(Huang et al., 2025) enables\ngradient-based selection via Gumbel-softmax but likewise processes full documents at every step, leaving the\nrepresentation mismatch and context length issues unresolved.\nAs motivated earlier, joint training of retrieval and generation in RAG systems is hindered by the non-\ndifferentiability of discrete document selection. In typical QA pipelines, the retriever reorders retrieved\ndocuments before generation (Yu et al., 2024; Dong et al., 2024), but discrete sampling operations prevent\ngradient backpropagation. In contrast, our framework uniquely combines compression and joint training: by\nemployinglength-flexible compressed vectorsin a shared latent space, we enable efficient differentiable\nselection while drastically reducing context length. Optimized solely through the generator‚Äôs language modeling\nloss, CLaRa ensures consistent training‚Äìinference alignment and efficient end-to-end learning without explicit\nretrieval supervision.\n7 Conclusion\nIn this paper, we address the challenge of compressing documents into high-quality implicit representations to\nenhance the performance of retrieval-augmented generation (RAG) systems that rely on document embeddings\nfor question answering. To this end, we design multiple pretraining objectives that leverage LLM prompting\nto construct diverse supervision signals, including QA pairs‚Äîcovering both simple and compositional reason-\ning‚Äîand paraphrased documents, encouraging the compressor to retain essential semantic information. We\nfurther introduce an efficient end-to-end training framework that unifies document representations across\nthe reranking and generation stages, leading to substantial improvements in retrieval accuracy and answer\nquality. Extensive experiments on multiple QA benchmarks demonstrate that embedding-based contextual\ncompression not only reduces input length and computation cost but also bridges the gap between retrieval\nand generation, enabling a more unified and semantically coherent RAG paradigm.\nLimitations\nCompressor Generalization.The current compressor is pretrained exclusively on Wikipedia data. To improve\ngeneralizability, future work may incorporate domain-adaptive pretraining objectives and leverage more diverse\ncorpora (e.g., code datasets (Wang et al., 2025)) to enhance representation robustness across modalities and\ndomains.\n11\nReasoning over Compressed Representations.While our study does not primarily focuses on reasoning based\non compressed representations, the compact and semantically dense nature of compressed representations\nmakes them a promising candidate for integration into reasoning-oriented or agentic RAG frameworks, such as\nSearch-R1 (Jin et al., 2025). A natural next step is to investigate whether such representations can function\nas efficient reasoning memory in multi-hop or planning-based RAG systems.\nModel Size.Our experiments are conducted on medium-scale models (Mistral-7B and Phi-4B). Larger models\nmay produce higher-quality document representations that better capture semantic nuances. An open question\nfor future work is whether there exists a model size threshold beyond which compressed representations\nsurpass raw text in supporting understanding and generation.\nGeneralization of Implicit Representations.This work focuses on leveraging unified representations for both\nunderstanding and generation within the RAG framework. Given the growing interest toward unified models\nthat jointly perform comprehension and generation (Zhang et al., 2025), extending compression-based methods\nto broader tasks‚Äîsuch as tool learning (Qu et al., 2025)‚Äîoffers a promising direction for developing more\ngeneral-purpose, reasoning-capable systems. Besides, linking implicit understanding with implicit reasoning\n(Hao et al., 2025; Kang et al., 2025) would be an interesting direction.\nReferences\nMohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia\nMohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin Asgari. Ask in any modality: A\ncomprehensive survey on multimodal retrieval-augmented generation. In Wanxiang Che, Joyce Nabende, Ekaterina\nShutova, and Mohammad Taher Pilehvar (eds.),Findings of the Association for Computational Linguistics: ACL 2025,\npp. 16776‚Äì16809, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5.\ndoi: 10.18653/v1/2025.findings-acl.861. URLhttps://aclanthology.org/2025.findings-acl.861/.\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-RAG: Learning to retrieve, generate,\nand critique through self-reflection. InThe Twelfth International Conference on Learning Representations, 2024.\nhttps://openreview.net/forum?id=hSyW5go0v8.\nOrlando Ayala and Patrice Bechard. Reducing hallucination in structured outputs via retrieval-augmented generation.\nIn Yi Yang, Aida Davani, Avi Sil, and Anoop Kumar (eds.),Proceedings of the 2024 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6:\nIndustry Track), pp. 228‚Äì238, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi:\n10.18653/v1/2024.naacl-industry.19. URLhttps://aclanthology.org/2024.naacl-industry.19/.\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multi-lingual,\nmulti-functionality, multi-granularity text embeddings through self-knowledge distillation, 2023.\nXin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao.\nxrag: extreme context compression for retrieval-augmented generation with one token. InProceedings of the 38th\nInternational Conference on Neural Information Processing Systems, NIPS ‚Äô24, Red Hook, NY, USA, 2025. Curran\nAssociates Inc. ISBN 9798331314385.\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts.\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.),Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing, pp. 3829‚Äì3846, Singapore, December 2023. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.emnlp-main.232. URLhttps://aclanthology.org/2023.emnlp-main.232/.\nYuhong Dai, Jianxun Lian, Yitian Huang, Wei Zhang, Mingyang Zhou, Mingqi Wu, Xing Xie, and Hao Liao.\nPretraining context compressor for large language models with embedding-based memory. In Wanxiang Che, Joyce\nNabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.),Proceedings of the 63rd Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 28715‚Äì28732, Vienna, Austria, July 2025.\nAssociation for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1394. URL\nhttps://aclanthology.org/2025.acl-long.1394/.\nGuanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Ji-Rong Wen, and Zhicheng Dou. Understand what LLM\nneeds: Dual preference alignment for retrieval-augmented generation. InTHE WEB CONFERENCE 2025, 2025.\nhttps://openreview.net/forum?id=2ZaqnRIUCV.\n12\nJialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F. Yang, and Anton Tsitsulin. Don‚Äôt forget to connect! improving rag\nwith graph-based reranking, 2024. URLhttps://arxiv.org/abs/2405.18414.\nShahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated evaluation of retrieval\naugmented generation, 2025. URLhttps://arxiv.org/abs/2309.15217.\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen\nWang. Retrieval-augmented generation for large language models: A survey, 2024. URLhttps://arxiv.org/abs/2312.10997.\nTao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context com-\npression in a large language model. InThe Twelfth International Conference on Learning Representations, 2024.\nhttps://openreview.net/forum?id=uREj4ZuGJE.\nShibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason E Weston, and Yuandong Tian. Training\nlarge language model to reason in a continuous latent space. 2025. https://openreview.net/forum?id=tG4SgayTtk.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop QA dataset for\ncomprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.),Proceedings\nof the 28th International Conference on Computational Linguistics, pp. 6609‚Äì6625, Barcelona, Spain (Online),\nDecember 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580.\nURLhttps://aclanthology.org/2020.coling-main.580/.\nSiyuan Huang, Zhiyuan Ma, Jintao Du, Changhua Meng, Weiqiang Wang, Jingwen Leng, Minyi Guo, and Zhouhan Lin.\nGumbel reranking: Differentiable end-to-end reranker optimization. In Wanxiang Che, Joyce Nabende, Ekaterina\nShutova, and Mohammad Taher Pilehvar (eds.),Proceedings of the 63rd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 7142‚Äì7161, Vienna, Austria, July 2025. Association for\nComputational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.354. URLhttps://aclanthology .org/\n2025.acl-long.354/.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand\nJoulin, Sebastian Riedel, and Edouard Grave. Atlas: few-shot learning with retrieval augmented language models.J.\nMach. Learn. Res., 24(1), January 2023. ISSN 1532-4435.\nBowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan O Arik, Dong Wang, Hamed Zamani, and Jiawei Han.\nSearch-r1: Training LLMs to reason and leverage search engines with reinforcement learning. InSecond Conference\non Language Modeling, 2025. https://openreview.net/forum?id=Rwhi91ideu.\nHaoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Nicklas Majamaki, Navdeep Jaitly, Yi-An Ma, and Lianhui Qin.\nLadir: Latent diffusion enhances llms for text reasoning, 2025. URLhttps://arxiv.org/abs/2510.04573.\nYuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, and Mikhail Burtsev. Cramming 1568 tokens into a single vector and\nback again: Exploring the limits of embedding space capacity. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova,\nand Mohammad Taher Pilehvar (eds.),Proceedings of the 63rd Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 19323‚Äì19339, Vienna, Austria, July 2025. Association for Computational\nLinguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.948. URLhttps://aclanthology.org/2025.acl-long.948/.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle\nEpstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-\nWei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for\nquestion answering research.Transactions of the Association for Computational Linguistics, 7:452‚Äì466, 2019. doi:\n10.1162/tacl_a_00276. URLhttps://aclanthology.org/Q19-1026/.\nKwun Hang Lau, Ruiyuan Zhang, Weijie Shi, Xiaofang Zhou, and Xiaojun Cheng. Reading between the timelines:\nRag for answering diachronic questions, 2025. URLhttps://arxiv.org/abs/2507.22917.\nQuinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, and Michael Carbin. Long context rag performance of large\nlanguage models, 2024. URLhttps://arxiv.org/abs/2411.03538.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler,\nMike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for\nknowledge-intensive nlp tasks. InProceedings of the 34th International Conference on Neural Information Processing\nSystems, NIPS ‚Äô20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\nXinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu,\net al. Rag-ddr: Optimizing retrieval-augmented generation using differentiable data rewards.arXiv preprint\narXiv:2410.13509, 2024a.\n13\nXinze Li, Sen Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan\nLiu, Maosong Sun, and Chenyan Xiong. RAG-DDR: Optimizing retrieval-augmented generation using dif-\nferentiable data rewards. InThe Thirteenth International Conference on Learning Representations, 2025.\nhttps://openreview.net/forum?id=Pnktu2PBXD.\nZhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. Retrieval augmented generation or\nlong-context LLMs? a comprehensive study and hybrid approach. In Franck Dernoncourt, Daniel Preo≈£iuc-Pietro,\nand Anastasia Shimorina (eds.),Proceedings of the 2024 Conference on Empirical Methods in Natural Language\nProcessing: Industry Track, pp. 881‚Äì893, Miami, Florida, US, November 2024b. Association for Computational\nLinguistics. doi: 10.18653/v1/2024.emnlp-industry.66. URLhttps://aclanthology.org/2024.emnlp-industry.66/.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez,\nJacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. RA-DIT: Retrieval-\naugmented dual instruction tuning. InThe Twelfth International Conference on Learning Representations, 2024.\nhttps://openreview.net/forum?id=22OTbutug9.\nZihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro. Chatqa:\nsurpassing gpt-4 on conversational qa and rag. InProceedings of the 38th International Conference on Neural\nInformation Processing Systems, NIPS ‚Äô24, Red Hook, NY, USA, 2025. Curran Associates Inc. ISBN 9798331314385.\nMaxime Louis, Herv√© D√©jean, and St√©phane Clinchant. PISCO: Pretty simple compression for retrieval-augmented\ngeneration. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.),Findings\nof the Association for Computational Linguistics: ACL 2025, pp. 15506‚Äì15521, Vienna, Austria, July 2025a.\nAssociation for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.800. URL\nhttps://aclanthology.org/2025.findings-acl.800/.\nMaxime Louis, Thibault Formal, Herv√© Dejean, and St√©phane Clinchant. Oscar: Online soft compression and reranking,\n2025b. URLhttps://arxiv.org/abs/2504.07109.\nThomas Merth, Qichen Fu, Mohammad Rastegari, and Mahyar Najibi. Superposition prompting: improving and\naccelerating retrieval-augmented generation. InProceedings of the 41st International Conference on Machine\nLearning, ICML‚Äô24. JMLR.org, 2024.\nJesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens. InProceedings of the\n37th International Conference on Neural Information Processing Systems, NIPS ‚Äô23, Red Hook, NY, USA, 2023.\nCurran Associates Inc.\nnostalgebraist. interpreting GPT: the logit lens, Aug 2020. URLhttps://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/\ninterpreting-gpt-the-logit-lens. LessWrong blog post.\nZhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor R√ºhle, Yuqing\nYang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. LLMLingua-2: Data distillation for efficient and\nfaithful task-agnostic prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.),Findings of\nthe Association for Computational Linguistics: ACL 2024, pp. 963‚Äì981, Bangkok, Thailand, August 2024. Association\nfor Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.57. URLhttps://aclanthology .org/2024.findings-acl.57/.\nRonak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. Rankzephyr: Effective and robust zero-shot listwise\nreranking is a breeze!, 2023. URLhttps://arxiv.org/abs/2312.02724.\nChangle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-rong Wen. Tool\nlearning with large language models: a survey.Front. Comput. Sci., 19(8), January 2025. ISSN 2095-2228. doi:\n10.1007/s11704-024-40678-2. URLhttps://doi.org/10.1007/s11704-024-40678-2.\nDavid Rau, Shuai Wang, Herv√© D√©jean, St√©phane Clinchant, and Jaap Kamps. Context embeddings for efficient answer\ngeneration in retrieval-augmented generation. InProceedings of the Eighteenth ACM International Conference on\nWeb Search and Data Mining, WSDM ‚Äô25, pp. 493‚Äì502, New York, NY, USA, 2025. Association for Computing\nMachinery. ISBN 9798400713293. doi: 10.1145/3701551.3703527. URLhttps://doi.org/10.1145/3701551.3703527.\nDevendra Singh Sachan, Siva Reddy, William L. Hamilton, Chris Dyer, and Dani Yogatama. End-to-end\ntraining of multi-document reader and retriever for open-domain question answering. In A. Beygelzimer,\nY. Dauphin, P. Liang, and J. Wortman Vaughan (eds.),Advances in Neural Information Processing Systems,\n2021. https://openreview.net/forum?id=5KWmB6JePx.\nZhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, and Zhaochun Ren. Generate-then-\nground in retrieval-augmented generation for multi-hop question answering. In Lun-Wei Ku, Andre Martins, and\nVivek Srikumar (eds.),Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n14\n(Volume 1: Long Papers), pp. 7339‚Äì7353, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\ndoi: 10.18653/v1/2024.acl-long.397. URLhttps://aclanthology.org/2024.acl-long.397/.\nZhengliang Shi, Lingyong Yan, Weiwei Sun, Yue Feng, Pengjie Ren, Xinyu Ma, Shuaiqiang Wang, Dawei Yin, Maarten\nde Rijke, and Zhaochun Ren. Direct retrieval-augmented optimization: Synergizing knowledge selection and language\nmodels, 2025. URLhttps://arxiv.org/abs/2505.03075.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via\nsingle-hop question composition.Transactions of the Association for Computational Linguistics, 10:539‚Äì554, 2022.\ndoi: 10.1162/tacl_a_00475. URLhttps://aclanthology.org/2022.tacl-1.31/.\nZora Zhiruo Wang, Akari Asai, Xinyan Velocity Yu, Frank F. Xu, Yiqing Xie, Graham Neubig, and Daniel Fried.\nCoderag-bench: Can retrieval augment code generation?, 2025. URLhttps://arxiv.org/abs/2406.14497.\nShangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan\nGuan, and Chun Jason Xue. Retrieval-augmented generation for natural language processing: A survey.CoRR,\nabs/2407.13193, 2024. URLhttps://doi.org/10.48550/arXiv.2407.13193.\nZilin Xiao, Qi Ma, Mengting Gu, Chun cheng Jason Chen, Xintao Chen, Vicente Ordonez, and Vijai Mohan. Metaembed:\nScaling multimodal retrieval at test-time with flexible late interaction, 2025. URLhttps://arxiv.org/abs/2509.18095.\nFangyuan Xu, Weijia Shi, and Eunsol Choi. RECOMP: Improving retrieval-augmented LMs with context compres-\nsion and selective augmentation. InThe Twelfth International Conference on Learning Representations, 2024.\nhttps://openreview.net/forum?id=mlJLVigNHp.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D.\nManning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang,\nJulia Hockenmaier, and Jun‚Äôichi Tsujii (eds.),Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 2369‚Äì2380, Brussels, Belgium, October-November 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-1259. URLhttps://aclanthology.org/D18-1259/.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant. Making retrieval-augmented language models\nrobust to irrelevant context. InThe Twelfth International Conference on Learning Representations, 2024.\nhttps://openreview.net/forum?id=ZS4m74kZpH.\nYue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro.\nRankRAG: Unifying context ranking with retrieval-augmented generation in LLMs. InThe Thirty-eighth Annual\nConference on Neural Information Processing Systems, 2024. https://openreview.net/forum?id=S1fc92uemC.\nZhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui\nWang, and Michael Bendersky. Inference scaling for long-context retrieval augmented generation. InThe Thirteenth\nInternational Conference on Learning Representations, 2025. https://openreview.net/forum?id=FSjIrOm1vz.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Eval-\nuating text generation with bert. InInternational Conference on Learning Representations, 2020.\nhttps://openreview.net/forum?id=SkeHuCVFDr.\nXinjie Zhang, Jintao Guo, Shanshan Zhao, Minghao Fu, Lunhao Duan, Jiakui Hu, Yong Xien Chng, Guo-Hua Wang,\nQing-Guo Chen, Zhao Xu, Weihua Luo, and Kaifu Zhang. Unified multimodal understanding and generation models:\nAdvances, challenges, and opportunities, 2025. URLhttps://arxiv.org/abs/2505.02567.\nJiawei Zhou and Lei Chen. Optimizing retrieval for rag via reinforced contrastive learning, 2025. URLhttps:\n//arxiv.org/abs/2510.24652.\nShengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. A setwise approach for effective and\nhighly efficient zero-shot ranking with large language models. InProceedings of the 47th International ACM\nSIGIR Conference on Research and Development in Information Retrieval, SIGIR ‚Äô24, pp. 38‚Äì47, New York, NY,\nUSA, 2024. Association for Computing Machinery. ISBN 9798400704314. doi: 10.1145/3626772.3657813. URL\nhttps://doi.org/10.1145/3626772.3657813.\nShengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin, and Guido Zuccon. Rank-r1: Enhancing reasoning in\nllm-based document rerankers via reinforcement learning, 2025. URLhttps://arxiv.org/abs/2503.06034.\nWeronika ≈Åajewska, Momchil Hardalov, Laura Aina, Neha Anna John, Hang Su, and Llu√≠s M√†rquez. Understanding\nand improving information preservation in prompt compression for llms, 2025. URLhttps://arxiv.org/abs/2503.19114.\n15\nA Gradients for Non-shared vs. Shared Representations in RAG\nStep 1:Lets xd be the retrieval score for queryxand documentd, and let\np(d|x) = exp(sxd)P\nd‚Ä≤‚ààC exp(sxd‚Ä≤) , p(y|x) =\nX\nd‚ààC\np(d|x)p(y|x, d),L=‚àílogp(y|x).(A.1)\nStep 2: product rule inside the sum.\n‚àÇ\n‚àÇsxd\n\u0010X\nd‚Ä≤\np(d‚Ä≤|x)p(y|x, d ‚Ä≤)\n\u0011\n=\nX\nd‚Ä≤\n‚àÇp(d‚Ä≤|x)\n‚àÇsxd| {z }\nsoftmax Jacobian\np(y|x, d‚Ä≤) +\nX\nd‚Ä≤\np(d‚Ä≤|x) ‚àÇp(y|x, d‚Ä≤)\n‚àÇsxd\n.(A.2)\nStep 3: softmax Jacobian.Forp(d ‚Ä≤|x) = esxd‚Ä≤\nP\nj esxj,\n‚àÇp(d‚Ä≤|x)\n‚àÇsxd\n=p(d ‚Ä≤|x)\n\u0000\n1[d‚Ä≤ =d]‚àíp(d|x)\n\u0001\n.\nTherefore\nX\nd‚Ä≤\n‚àÇp(d‚Ä≤|x)\n‚àÇsxd\np(y|x, d‚Ä≤) =\nX\nd‚Ä≤\np(d‚Ä≤|x)\n\u0000\n1[d‚Ä≤ =d]‚àíp(d|x)\n\u0001\np(y|x, d‚Ä≤)(A.3)\n=p(d|x)p(y|x, d)‚àíp(d|x)\nX\nd‚Ä≤\np(d‚Ä≤|x)p(y|x, d ‚Ä≤)(A.4)\n=p(d|x)\n\u0000\np(y|x, d)‚àíp(y|x)\n\u0001\n.(A.5)\nStep 4: put together.Plugging equation A.5 into equation A.2 and then equation A.1 gives\n‚àÇL\n‚àÇsxd\n=‚àí 1\np(y|x)\nÔ£Æ\nÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞\np(d|x)\n\u0000\np(y|x, d)‚àíp(y|x)\n\u0001\n| {z }\n(I) probability path\n+\nX\nd‚Ä≤\np(d‚Ä≤|x) ‚àÇp(y|x, d‚Ä≤)\n‚àÇsxd\n| {z }\n(II) representation/generation path\nÔ£π\nÔ£∫Ô£∫Ô£∫Ô£∫Ô£ª\n.(A.6)\nStep 5: common simplification (assumption).If the generator‚Äôs conditional p(y|x, d‚Ä≤)depends on sxd only when\nd‚Ä≤ = d (e.g., each conditional uses its own selected document; non-shared case gives it zero), then the second\nsum reduces to a single term:\nX\nd‚Ä≤\np(d‚Ä≤|x) ‚àÇp(y|x, d‚Ä≤)\n‚àÇsxd\n=p(d|x) ‚àÇp(y|x, d)\n‚àÇsxd\n.\nUnder this widely-used assumption, equation A.6 becomes\n‚àÇL\n‚àÇsxd\n=‚àí 1\np(y|x)\nh\np(d|x)\n\u0000\np(y|x, d)‚àíp(y|x)\n\u0001\n| {z }\n(I) probability path\n+p(d|x) ‚àÇp(y|x, d)\n‚àÇsxd| {z }\n(II) representation/generation path\ni\n. (A.7)\nRemark (more general shared-conditioning).If the generator conditions on a mixture r =P\nj œÄj(s) zj (so every\np(y|x, d‚Ä≤)shares thesame r), then ‚àÇp(y|x,d ‚Ä≤)\n‚àÇsxd\nis the same for alld‚Ä≤ andP\nd‚Ä≤ p(d‚Ä≤|x) ‚àÇp(y|x,d ‚Ä≤)\n‚àÇsxd\n= ‚àÇp(y|x,r)\n‚àÇsxd\n. Both\nforms are consistent; the boxed formula corresponds to the per-document conditional view.\nTerm (II) is present if the generator‚Äôs conditionalp(y|x, d)depends (directly or indirectly) onsxd.\n16\nCase A: Non-shared representations (retrieverÃ∏=generator)\nHere the generator consumes raw tokens or an independent encoder, hencep(y|x, d )does not depend on sxd:\n‚àÇp(y|x, d)\n‚àÇsxd\n= 0.(A.8)\nPlugging into equation A.7 gives the complete gradient:\n‚àÇL\n‚àÇsxd\n=‚àí 1\np(y|x) p(d|x)\n\u0000\np(y|x, d)‚àíp(y|x)\n\u0001\n. (A.9)\nThis expression already accounts for the softmax coupling viap(y|x )and is more accurate than writing only\n‚àí 1\np(y|x) p(y|x, d)‚àÇp(d|x)/‚àÇs xd.\nCase B: Shared representations (retriever=generator)\nWhen retriever and generator share embeddings,p(y|x, d )depends on sxd through the generator‚Äôs conditioning\nvector. A common differentiable conditioning is\nœÄj = exp(sxj/œÑ)P\n‚Ñì exp(sx‚Ñì/œÑ) , r=\nX\nj‚ààC\nœÄj zj, p(y|x, d)‚â°p(y|x, r),(A.10)\nwhereœÑ >0is the temperature,z j are document embeddings, andris fed to the generator.\nLet\ng‚âú ‚àÇlogp(y|x, r)\n‚àÇr =\nX\nt\n‚àÇlogp(y t |y <t, x, r)\n‚àÇr .(A.11)\nUsing the softmax Jacobian,\n‚àÇr\n‚àÇsxd\n=\nX\nj\n‚àÇœÄj\n‚àÇsxd\nzj = 1\nœÑ œÄd (zd ‚àír).(A.12)\nBy chain rule,\n‚àÇp(y|x, d)\n‚àÇsxd\n=p(y|x, r)g ‚ä§ ‚àÇr\n‚àÇsxd\n= p(y|x, r)\nœÑ œÄd g‚ä§\u0000\nzd ‚àír\n\u0001\n. (A.13)\nSubstituting equation A.13 into equation A.7 yields the full shared-representation gradient:\n‚àÇL\n‚àÇsxd\n=‚àí 1\np(y|x)\nh\np(d|x)\n\u0000\np(y|x, d)‚àíp(y|x)\n\u0001\n+ p(r|x)p(y|x, r)\nœÑ œÄd g‚ä§\u0000\nzd ‚àír\n\u0001i\n. (A.14)\nStraight-through (ST) note.If the forward pass uses hard top-kselection (argmax/indices) but the backward\npass adopts the softmax gradient (ST estimator), then formulas equation A.12‚Äìequation A.14 remain the\ncorrect backpropagation rules (withœÄcomputed from the scores for the backward pass).\nOptional: Cosine-similarity score backpropagation\nIf the score is cosine similarity\nsxd = q‚ä§zd\n‚à•q‚à• ‚à•zd‚à• ,(A.15)\n17\nAlgorithm 1Differentiable Top-kSelection with Straight-Through Estimator in CLaRA\n1:Input:Similarity scoress‚ààR B√óD , temperatureœÑ, number of selectionsk\n2:Output:Selection tensorZ‚ààR B√ók√óD and top-kindices{r j }k\nj=1\n3:Àús‚Üês/max(œÑ,10 ‚àí6)\n4: InitializeZ hard, Zsoft ‚Üê0 B√ók√óD , taken‚Üê0 B√óD\n5:forj= 1tokdo\n6:(1) Hard selection:r j ‚Üêarg max i Àús(:, i)on unmasked candidates\n7:Z hard[:, j, rj ]‚Üê1\n8:(2) Soft selection:mask‚Üê1‚àíSG(taken)\n9: logits j ‚ÜêÀús+ log(mask+Œµ)\n10:p j ‚Üêsoftmax(logits j )\n11:Z soft[:, j,:]‚Üêp j\n12: taken‚Üêmin(taken+Z hard[:, j,:],1)\n13:end for\n14:(3) Straight-through estimator:Z‚ÜêZ hard + (Zsoft ‚àíSG(Z soft))\n15:return(Z,{r j }k\nj=1 )\nthen the required Jacobians are\n‚àÇsxd\n‚àÇq = 1\n‚à•q‚à• ‚à•zd‚à•\n\u0012\nzd ‚àís xd\nq\n‚à•q‚à•2 ‚à•zd‚à•\n\u0013\n, ‚àÇsxd\n‚àÇzd\n= 1\n‚à•q‚à• ‚à•zd‚à•\n\u0012\nq‚àís xd\nzd\n‚à•zd‚à•2 ‚à•q‚à•\n\u0013\n.(A.16)\nHence\n‚àÇL\n‚àÇq =\nX\nd‚ààC\n‚àÇL\n‚àÇsxd\n‚àÇsxd\n‚àÇq , ‚àÇL\n‚àÇzd\n= ‚àÇL\n‚àÇsxd\n‚àÇsxd\n‚àÇzd\n.(A.17)\nSimple QA Complex QA Paraphrase Doc\nNum. 2,000,000 2,000,000 1,966,291\nAvg.pairs 7.80 4.62 1.00\nAvg.inp 95.56 95.56 95.56\nAvg.out 158.18 253.90 108.67\nTable 9Pretraining data statistics for SCP. The table reports the total number of training examples (Num.), average\nnumber of generated QA pairs or documents (Avg.pairs), average input document length (Avg.inp) and average\ngenerated text length (Avg.out) for Simple QA, Complex QA, and Paraphrased Documents..\nDatasets Training Data Size Evaluation Data Size\nNature Question 58,622 6,489\nHotpotQA 90,185 7,384\nMusiQue 168,745 2,417\n2WikiMultiHopQA 167,454 12,576\nTable 10Statistics of experimental datasets.\nB Detailed experimental setup\nB.1 Datasets\nThepretrainingcorpus consists of 2M documents and their corresponding 2MSimpleQAsets, 2MComplexQA\nsets, and 2M paraphrased documents. Detailed statistics on data composition and distribution are provided\nin Table 9.\nDuring theinstruction tuningstage of compression learning, we use question data fromCOCOM(Rau\net al., 2025) , which contains 453k questions. We employ theMistral-7B model and retrieve the top-5 most\n18\nModels CR NQ HotpotQA Musique 2Wiki A verage\nNormal\nAutocompressor 1x 17.24 14.61 3.81 19.89 13.89\nxrag 128x 32.35 25.16 3.64 28.79 22.48\ncoconum 16x 24.12 21.48 3.52 24.48 18.40\npcc 16x 31.38 22.29 3.43 19.47 19.14\nllmlingua-2 4x 47.53 37.05 9.02 44.35 34.49\npisco 16x 54.39 41.94 10.09 44.88 37.83\nMistral-7B w/o BGE retrieval 1x 35.01 27.55 5.38 38.45 26.6\nMistral-7B w/ BGE retrieval 1x 54.58 42.94 8.94 44.24 37.67\nSCP-Mistral-7B\n4x57.05+2.4745.09+2.1510.34+1.4046.94+2.7039.86+2.19\n16x 55.56+0.9843.72+0.7810.55+1.6146.00+1.7638.96+1.29\n32x 54.64+0.0643.52+0.5810.55+1.6146.58+2.3438.82+1.15\n64x 54.18-0.4042.17-0.7710.17+1.2347.03+2.7938.39+0.72\n128x 53.36-1.2241.37-1.5710.26+1.3246.40+2.1637.85+0.18\n256x 52.84-1.7440.00-2.9410.38+1.4446.31+2.0737.38-0.29\nPhi4-mini w/o BGE retrieval 1x 18.77 21.10 4.05 30.26 18.55\nPhi4-mini w/ BGE retrieval 1x 48.14 37.78 8.11 35.11 32.28\nSCP-Phi4-mini\n4x53.31+5.1742.36+4.588.73+0.6245.22+10.1137.40+5.12\n16x 51.96+3.8240.86+3.088.61+0.5044.27+9.1636.42+4.14\n32x 49.30+1.1638.62+0.847.70-0.4143.71+8.6034.83+2.55\n64x 45.72-2.4235.75-2.036.50-1.6143.96+8.8532.98+0.70\n128x 43.09-5.0533.92-3.866.87-1.2443.70+8.5931.90-0.38\n256x 42.73-5.4134.02-3.766.87-1.2443.75+8.6431.84-0.44\nOracle\nAutocompressor 1x 29.47 19.24 7.16 26.74 20.65\nxrag 128x 42.60 30.21 7.03 30.94 27.70\ncoconum 16x 25.61 21.72 3.64 24.63 18.90\npcc 16x 49.62 34.56 18.25 27.56 32.50\nllmlingua-2 4x 63.99 52.42 27.47 53.92 49.45\npisco 16x 73.44 66.53 33.80 60.45 58.55\nMistral-7B w/ BGE retrieval 1x 71.64 70.77 45.72 68.83 64.24\nSCP-Mistral-7B\n4x76.50+4.8673.81+3.0446.26+0.5470.48+1.6566.76+2.52\n16x 75.48+3.8470.79+0.0243.15-2.5766.16-2.6763.90-0.34\n32x 73.77+2.1369.51-1.2638.31-7.4164.54-4.2961.53-2.71\n64x 71.90+0.2666.22-4.5534.96-10.7661.55-7.2858.66-5.58\n128x 69.96-1.6862.09-8.6830.86-14.8659.08-9.7555.50-8.74\n256x 68.82-2.8259.93-10.8426.19-19.5356.50-12.3352.86-11.38\nPhi4-mini w/ BGE retrieval 1x 66.10 64.06 37.07 52.69 54.98\nSCP-Phi4-mini\n4x73.67+7.5772.41+8.3540.13+3.0664.22+11.5362.61+7.63\n16x 73.17+7.0770.26+6.2038.39+1.3263.15+10.4661.24+6.26\n32x 69.90+3.8065.32+1.2631.77-5.3058.52+5.8356.38+1.40\n64x 64.72-1.3857.79-6.2723.54-13.5353.11+0.4249.79-5.19\n128x 60.44-5.6651.52-12.5419.28-17.7950.29-2.4045.38-9.60\n256x 60.12-5.9851.54-12.5219.61-17.4650.33-2.3645.40-9.58\nTable 6Compressor performance on four QA datasets. The best performance is highlighted in bold. We show the\nabsolute performance change (¬±) of our method under different compression rates relative to its corresponding w/\nretrieval setting. CR denotes compression rate.\nsimilar documents from theWikipedia-2021 corpus using dense retrieval. Given each query and its retrieved\ndocuments, the model is prompted to generate the corresponding answer, which serves as the gold target for\ninstruction tuning.\nForend-to-endtraining, we use the training set of each benchmark individually, except for MuSiQue. Since\nMuSiQue is more challenging and difficult to converge when trained alone, we construct its training set by\ncombining the training samples from HotpotQA, 2Wiki, and MuSiQue. For each query, we first obtain its\npositive documents, and then retrieve additional documents from the corpus using theBGE-large-en-v1.5\nmodel until we collect a total of 20 candidates. This ensures that the gold answer remains inferable from at\nleast one of the selected documents during end-to-end optimization. Table 10 summarizes the data statistics.\n19\nModels CR Retrieval Mode NQ HotpotQA Musique 2Wiki\nF1 EM F1 EM F1 EM F1 EM\nPrompting-based Method\nGenGround* 1x Normal 42.31 40.60 44.71 41.27 24.36 20.77 42.58 39.61\nIn-context RAG* 1x Normal 44.69 38.07 41.27 37.14 20.11 16.78 41.02 38.51\nRetrieval tuning\nRECOMP* 1x Normal 42.67 37.47 42.72 38.72 24.96 17.34 38.26 32.17\nDPA-RAG* 1x Normal 44.31 37.29 40.53 37.15 20.36 18.45 39.66 39.02\nLLM Fine-tuning\nRetRobust* 1x Normal 43.82 37.03 40.54 35.59 18.16 18.11 39.11 38.65\nChatQA* 1x Normal 34.54 23.64 44.60 33.40 17.05 16.64 31.90 26.80\nSelf-RAG* 1x Normal 31.63 29.74 27.30 16.30 21.50 9.43 27.33 23.52\nEnd-to-end optimization\nDDR-RAG* 1x Normal 28.76 40.74 35.44 31.71 10.57 13.54 38.40 35.44\nDRO-Mistral-7B* 1x Normal 51.01 42.41 47.87 40.37 25.32 21.36 43.65 42.12\nPretraining-initialized\nCLaRa-Mistral-7B\n4x\nNormal\n40.62 31.21 39.53 29.54 14.53 6.16 42.59 38.49\n16x 41.75 32.24 44.37 33.72 15.36 6.99 43.47 39.50\n32x 40.68 31.36 41.84 31.26 15.32 6.66 43.23 38.98\n64x 41.58 31.38 41.62 31.12 14.78 6.16 42.64 38.40\n128x 42.04 31.78 42.26 31.78 15.53 6.37 41.80 37.37\n256x 42.90 32.58 41.32 30.44 15.44 6.41 41.96 37.60\n4x\nOracle\n77.80 70.52 77.66 64.83 41.59 30.33 73.20 69.14\n16x 73.81 65.74 69.57 56.76 31.15 21.18 65.90 61.31\n32x 72.03 63.65 70.91 57.07 33.40 22.22 66.32 61.12\n64x 68.18 59.56 67.64 53.22 28.43 17.42 62.53 57.02\n128x 68.66 59.25 66.51 52.30 28.44 16.67 64.82 58.97\n256x 66.85 57.17 63.43 49.08 27.44 16.92 62.96 57.35\nCLaRa-Phi4-mini\n4x\nNormal\n39.69 30.41 37.10 27.33 15.20 6.08 38.43 34.26\n16x 31.93 23.38 37.21 27.22 14.30 4.84 40.03 35.62\n32x 30.70 21.78 37.14 26.99 13.26 4.39 38.15 33.82\n64x 28.88 19.91 34.98 25.07 13.31 4.55 37.74 33.57\n128x 29.26 19.85 34.73 24.95 13.07 4.01 36.41 32.23\n256x 29.92 20.53 34.10 24.62 13.07 4.22 35.98 31.61\n4x\nOracle\n61.07 52.15 59.82 46.99 25.87 15.76 56.84 52.12\n16x 65.09 55.96 57.87 45.26 21.09 11.25 55.75 50.41\n32x 62.35 52.07 51.06 38.33 20.98 10.92 50.68 45.41\n64x 58.51 47.08 55.47 41.62 23.89 12.99 53.90 48.26\n128x 56.13 44.52 52.68 38.49 20.74 9.85 49.97 44.11\n256x 54.58 43.24 51.62 37.77 21.45 9.89 48.46 42.63\nInstruction-tuned-initialized\nCLaRa-Mistral-7B\n4x\nNormal\n48.21 38.16 45.93 35.12 17.49 8.11 47.18 43.11\n16x 50.89 41.02 47.62 36.67 18.01 8.44 44.66 40.48\n32x 49.72 39.88 45.73 34.85 16.83 7.82 42.57 38.41\n64x 50.91 41.07 45.68 34.74 16.76 7.65 40.34 35.91\n128x 51.41 41.27 44.63 33.88 15.75 7.03 40.55 36.12\n256x 50.57 40.39 43.02 32.26 16.02 6.99 40.10 35.77\n4x\nOracle\n75.63 67.64 69.66 56.92 33.19 22.42 73.86 69.74\n16x 71.54 63.29 71.17 57.54 30.77 20.56 60.37 55.73\n32x 69.75 65.17 68.87 55.20 28.87 18.45 64.38 59.32\n64x 68.17 59.04 66.64 52.87 27.30 16.96 60.98 55.59\n128x 66.95 57.61 64.09 54.63 26.11 15.97 62.34 56.64\n256x 65.60 55.65 61.79 47.74 27.67 17.05 59.40 53.75\nCLaRa-Phi4-mini\n4x\nNormal\n41.86 31.96 39.44 29.32 15.70 5.59 37.63 33.40\n16x 42.17 32.61 42.77 32.00 15.84 6.08 36.69 32.47\n32x 39.14 29.45 42.59 31.47 15.55 5.71 41.47 36.68\n64x 36.91 27.09 38.90 28.02 14.08 4.88 39.52 34.98\n128x 36.26 26.34 36.39 26.44 14.70 5.42 37.14 32.85\n256x 37.58 27.48 35.84 25.73 13.66 4.92 36.26 32.11\n4x\nOracle\n55.53 45.94 55.28 43.24 25.96 15.14 55.57 50.18\n16x 58.62 48.90 56.47 43.45 23.07 12.49 56.85 51.57\n32x 61.15 50.45 56.31 43.13 21.28 11.29 51.21 45.70\n64x 57.63 46.62 52.39 38.94 22.38 11.63 48.11 42.83\n128x 56.26 44.77 50.74 37.68 22.27 11.79 47.64 42.31\n256x 54.55 43.09 50.00 36.78 20.92 11.01 46.85 41.66\nTable 7End-to-End QA Performance. * indicates that the results are reported from the DRO paper. CR means\ncompression rate.\n20\nModels CR NQ HotpotQA Musique 2Wiki\nR@1 R@3 R@5 R@1 R@3 R@5 R@1 R@3 R@5 R@1 R@3 R@5\nBM25 1x 6.57 21.89 35.99 25.12 48.86 62.09 13.00 29.12 39.13 15.23 37.06 51.40\nBGE-reranker 1x 11.18 33.56 47.78 28.53 67.91 85.93 18.32 42.45 54.13 22.21 54.95 68.32\nRankZephyr-7B 1x 16.00 40.20 52.40 38.00 63.70 75.90 21.04 40.61 50.65 33.40 61.55 74.69\nSetwise 1x 12.60 38.20 52.10 30.40 60.30 74.60 18.50 40.30 51.60 26.20 57.99 71.61\nRank-R1 1x 8.95 30.60 46.72 24.87 56.56 73.06 16.77 38.49 51.68 24.27 59.02 77.02\nPretraining-initialized\nSup-Mistral-7B\n4x 31.57 62.34 74.96 46.33 71.76 82.84 32.35 50.30 60.02 42.38 74.51 85.02\n16x 30.66 61.05 73.30 44.00 69.66 80.45 29.44 45.07 54.63 40.03 65.48 77.27\n32x 30.19 60.89 73.93 45.69 73.76 83.80 31.34 48.20 57.86 42.08 68.09 79.41\n64x 29.88 60.38 73.53 45.92 74.35 84.46 31.32 46.91 55.93 42.13 68.26 79.52\n128x 29.02 59.25 72.33 44.79 73.76 83.76 29.42 45.97 55.21 40.65 65.61 76.97\n256x 27.54 56.67 70.00 44.50 73.89 83.81 29.13 46.33 56.04 39.99 64.87 76.22\nCLaRa-Mistral-7B\n4x 32.62 63.71 76.38 47.07 90.90 96.21 30.45 59.99 72.46 34.37 68.08 79.13\n16x 28.45 58.97 71.88 42.01 77.80 87.85 22.57 44.34 58.83 32.04 67.12 82.50\n32x 28.06 59.68 73.21 43.84 81.32 90.32 28.22 56.19 70.19 32.36 64.96 81.01\n64x 27.69 59.11 72.79 43.17 79.35 89.54 24.96 49.13 62.37 32.84 63.45 79.40\n128x 28.17 59.64 73.30 40.70 73.60 84.70 20.74 41.29 54.86 31.80 63.41 79.48\n256x 25.33 55.62 69.79 39.68 71.38 83.34 19.58 38.57 51.33 30.81 56.56 72.59\nSup-Phi4-mini\n4x 24.22 52.18 66.15 39.85 66.57 78.94 26.13 42.16 51.27 37.43 62.27 73.49\n16x 28.16 57.82 71.13 42.18 67.66 79.18 28.48 43.75 53.55 39.96 55.62 66.99\n32x 27.66 57.96 71.69 41.66 69.12 80.30 27.87 42.92 51.16 39.11 61.05 71.51\n64x 27.40 57.76 72.01 41.40 70.39 81.34 28.11 44.27 53.46 38.27 61.67 73.13\n128x 27.45 57.08 70.61 41.69 70.47 81.28 27.62 42.85 51.41 37.85 63.81 74.89\n256x 25.67 54.20 67.85 40.82 67.54 78.62 27.03 41.45 49.93 36.78 60.32 71.13\nCLaRa-Phi4-mini\n4x 8.58 28.21 44.61 18.38 42.90 60.01 10.56 26.81 40.08 17.05 38.03 52.63\n16x 20.94 48.89 64.34 23.90 49.17 64.38 15.07 30.79 42.49 21.16 46.83 64.10\n32x 27.63 58.37 72.37 28.64 55.48 70.00 16.43 34.31 46.55 30.30 58.89 75.98\n64x 29.17 60.08 73.83 36.25 65.67 78.44 16.38 33.36 46.23 27.74 53.69 69.63\n128x 29.63 60.54 74.02 37.94 63.96 75.93 17.67 34.90 46.39 30.79 58.53 74.64\n256x 27.50 57.90 71.58 37.10 63.30 75.23 17.67 34.13 44.98 31.79 60.53 76.90\nInstruction-tuned-initialized\nSup-Mistral-7B\n4x 28.33 58.52 71.96 42.40 65.93 77.40 27.87 45.47 55.39 39.90 62.02 74.35\n16x 28.20 57.24 69.57 42.24 67.91 79.33 26.80 42.33 50.68 39.22 59.77 70.38\n32x 27.56 56.70 69.58 44.88 71.02 81.54 29.29 43.86 52.22 42.21 55.47 66.56\n64x 25.70 54.11 66.80 44.94 73.32 84.24 29.02 44.21 53.28 42.09 54.68 66.03\n128x 25.05 53.12 65.73 45.14 74.56 85.18 28.11 43.18 52.40 42.27 58.47 69.94\n256x 25.15 52.61 65.27 44.60 73.95 85.18 28.69 44.32 53.25 41.89 52.64 63.77\nCLaRa-Mistral-7B\n4x 24.66 55.27 69.82 25.73 52.20 68.63 18.06 37.67 50.27 28.51 57.74 73.00\n16x 23.66 52.43 66.79 35.85 67.69 81.13 18.95 37.21 50.71 12.85 32.17 47.90\n32x 21.54 49.50 65.13 35.78 65.94 80.11 16.75 36.93 50.48 17.14 39.59 55.75\n64x 20.77 49.52 64.71 33.65 63.93 78.47 16.13 34.13 46.41 17.40 38.10 51.61\n128x 20.07 47.19 61.98 32.24 61.83 77.36 13.77 29.70 41.93 19.43 42.62 57.31\n256x 19.39 46.91 62.66 29.71 56.66 71.58 15.84 31.92 43.57 20.12 42.26 56.29\nSup-Phi4-mini\n4x 22.38 49.29 62.81 39.91 66.12 77.59 26.11 41.34 50.39 37.79 61.89 72.96\n16x 23.85 52.04 65.97 40.92 65.97 77.75 26.47 42.17 51.07 38.52 53.51 64.79\n32x 23.72 52.09 65.43 41.53 67.81 79.44 27.41 41.40 50.23 39.79 51.25 60.99\n64x 22.34 49.46 63.40 40.93 68.95 80.57 26.17 42.17 51.14 38.47 49.72 58.88\n128x 22.80 50.64 63.30 40.47 67.99 80.36 27.05 41.82 50.21 38.95 50.31 59.78\n256x 22.27 49.31 62.15 39.76 65.61 78.39 26.54 41.06 48.39 37.68 49.09 56.86\nCLaRa-Phi4-mini\n4x 3.98 17.02 31.76 13.19 33.04 48.40 7.60 21.09 31.03 20.40 39.25 52.00\n16x 8.15 25.82 40.84 18.00 41.31 57.10 7.82 18.24 28.19 21.16 40.80 51.96\n32x 16.94 42.64 58.30 31.30 58.02 71.88 13.71 29.99 40.74 29.85 53.24 66.04\n64x 17.63 44.29 59.33 32.09 59.98 74.35 15.93 32.42 44.22 29.77 56.59 71.59\n128x 21.90 50.34 65.14 30.53 55.15 68.30 12.81 27.30 37.66 25.54 47.53 61.28\n256x 19.32 45.89 60.39 31.18 56.05 69.80 12.24 26.19 37.40 27.54 51.71 64.10\nTable 8Retrieval performance (Recall@1/3/5) on theMistral-7BandPhi-4-minimodel across different reranking\nmethods under various compression ratios (CR) and initialization settings on four QA datasets.Sup-denotes models\ntrained with labeled data using contrastive learning for the reranker.\n21\nB.2 Models\nFor document retrieval, we employBGE-large-en-v1.51 as the retriever for coarse ranking. Unless otherwise\nspecified, we adoptMistral-7B-Instruct-v0.22 as the default backbone for all experiments. Additionally,\nwe evaluate the proposed method onPhi-4-mini-instruct3 to assess its generalization across different\nmodel families. On top of the backbone model, we implement three LoRA modules: acompressor, aquery\nreasoner, and ageneration module.\nB.3 Evaluation Metrics\nFollowing previous studies (Cheng et al., 2025; Louis et al., 2025a), we evaluate the compressor using the\nCover Exact Match (ACC)metric, which measures whether the ground-truth answer is included in the\ngenerated output. For the reranker, we reportRecall@k(k‚àà { 1, 3, 5}), defined as the proportion of positive\ndocuments appearing within the top-k ranked results. For the generation model, we adopt two standard QA\nmetrics:Exact Match (EM)andF1. The EM score measures the percentage of predicted answers that\nexactly match the gold answers, while the F1 score computes the token-level overlap between predictions and\nreferences, reflecting the harmonic mean of precision and recall.\nHyperparameter Value\nLR Scheduler cosine\nOptimizer AdamW\nEpochs 1\nLoRA Layers (r) all-linear\nLoRA Rank (r) 16\nLoRA Dropout 0.1\nLoRA Alpha 32\nLoRA Rank (r) 16\nWarmup Ratio 0.03\nMax Gradient Norm 1.0\nDocuments Max Tokens 256\nCompression learning\nŒª0.1\nBatch Size 128\nLearning Rate (LR)1√ó10 ‚àí4\nEnd-to-end learning\nBatch Size 32\nLearning Rate (LR)5√ó10 ‚àí6\nTable 11Hyperparameter settings used in our experiments.\nB.4 Implementation Details\nTable 11 summarizes the hyperparameters used for all LoRA modules and training stages. Specifically,\nwe employ separate configurations for the compression learning, and end-to-end training phases. During\nend-to-end learning, both the query reasoner and the generator are initialized from the compressor-trained\ncheckpoints. Following Shi et al. (2025), for each queryx, we first retrieve the top-20 documents from the\ncorpus using BGE-large-en-v1.5, obtain their corresponding compressed representations, and then pass\nthem along with the query into the query reasoner to identify the top-k (k = 5) ranked documents, which are\nsubsequently fed into the generator.\nFor corpus preprocessing, each document is segmented into chunks of 256 tokens. We extensively evaluate our\nmodel under different compression ratiosœÅ‚àà { 4, 16, 32, 64, 128, 256}, where the number of memory tokens\nis computed as256/œÅ. All experiments are conducted on8√ó 100H100 GPUs. Unless otherwise stated, all\ntraining runs are performed for a single epoch.\n1https://huggingface.co/BAAI/bge-large-en-v1.5\n2https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n3https://huggingface.co/microsoft/Phi-4-mini-instruct\n22\nB.5 Baselines\nIn this section, we provide detailed descriptions of all baseline methods used for comparison under different\nexperimental settings. We categorize them into three groups: (1)compression baselines, (2)retrieval and\nreranking baselines, and (3)end-to-end QA baselines.\nB.5.1 Compression Baselines\nAutoCompressor. (Chevalier et al., 2023)This method segments a long document into chunks, appends a<Sum>\ntoken at the end of each chunk, and trains the model to produce a fixed number of summary vectors. During\ntraining, the model is fine-tuned with a standard language modeling cross-entropy loss, with a stop-gradient\napplied to past summary vectors. At inference time, the model first compresses and then reuses the summaries,\nachieving efficient long-context reasoning at significantly reduced cost.\nXRAG. (Cheng et al., 2025)XRAG treats retrieved document embeddings as an additionalretrieval modality,\nmapping them into the language model‚Äôs representation space via a lightweight projection layer. This enables\nretrieval-augmented generation with as few as a single ‚Äúdocument token.‚Äù XRAG adopts a two-stage training\nstrategy: (1)Paraphrase Pretrainingto align document embeddings with textual semantics, and (2)Context-\nAware Instruction Tuningwith self-distillation to optimize retrieval utilization. Only the projection layer is\ntrained, while both the retriever and language model remain frozen, achieving compression ratios up to 178√ó.\nCOCOM. (Rau et al., 2025)COCOM maps each retrieved document into a compact sequence ofcontext\nembeddings(e.g., compressing hundreds of tokens into 4‚Äì128 embeddings), which reduces input length and\naccelerates generation. It jointly trains a compressor and a generator with two objectives: (i) an auto-encoding\nreconstruction loss to preserve semantic information, and (ii) a conditional generation loss to ensure high-\nquality answers from compressed contexts. The framework also supports multi-document compression and\ncross-document fusion, and offers a lightweight variant (COCOM-light) using BERT as the compressor.\nPCC. (Dai et al., 2025)PCC consists of an encoder and a transformer-based converter. The encoder extracts\ncompact semantic representations, while the converter adjusts their dimensionality and semantics through\ntwo MLP layers so that the compressed memory can be directly fed into any LLM. The model is pretrained\n(with the LLM frozen) using a combination ofauto-encoding reconstructionandauto-regressive completion\ntasks to retain generation-relevant information. Domain-specific fine-tuning is then performed on limited data\nfor RAG QA, ICL reasoning, and dialogue tasks.\nLLMLingua-2. (Pan et al., 2024)LLMLingua-2 constructs a large-scale extractive compression dataset using\nGPT-4-generated high-fidelity summaries. It formulates compression as a token-level binary classification\nproblem (keep or remove), where a bidirectional Transformer encoder (e.g., XLM-RoBERTa) estimates the\nretention probability of each token. Tokens are ranked by their probabilities to achieve 2‚Äì5√ócompression\nwhile maintaining semantic completeness.\nPISCO. (Louis et al., 2025a)PISCO introduces trainablememory tokensappended to the document, jointly\nfine-tuned with LoRA adapters to compress text by up to 1/16 of its original length. It employssequence-\nlevel knowledge distillation (SKD)from teacher-generated answer sequences to ensure consistency between\ncompressed and uncompressed outputs.\nB.5.2 Retrieval and Reranking Baselines\nBM25.A classical lexical retrieval method that scores each document based on term frequency, inverse\ndocument frequency, and document length normalization.\nBGE-Reranker. (Chen et al., 2023)A recent large-scale, general-purpose reranker that directly predicts the\nrelevance score between a query and each candidate document, used to reorder initial retrieval results.\n23\nRankZephyr. (Pradeep et al., 2023)A 7B-parameter open-source reranker distilled in two stages from RankGPT-\n3.5 and RankGPT-4. It integrates variable-window training, input order shuffling, and teacher-guided ranking\ndata, achieving robust performance under varying document counts and ranking conditions. During inference,\nRankZephyr performs iterative sliding-window ranking using prompt-decoder style generation.\nSetwise. (Zhuang et al., 2024)Unlike pairwise reranking, Setwise compares multiple candidate documents\nin a single inference step, greatly reducing LLM calls and prompt length. It leverages classical sorting\nalgorithms (e.g., heap or bubble sort) and directly estimates relevance probabilities from model logits, avoiding\nstep-by-step list generation.\nRank-R1. (Zhuang et al., 2025)A reinforcement learning-based reranking framework that enhances LLM\nreasoning capabilities for document ranking. Built upon the Setwise ranking paradigm, it introduces explicit\nreasoninginstructionsbeforeanswergenerationandoptimizesthemodelviaGroup Relative Policy Optimization\n(GRPO). The model is trained only with queries and relevance labels, and receives reward signals based on\nprediction correctness and format compliance.\nB.5.3 End-to-End QA Baselines\nGenGround. (Shi et al., 2024)This method decomposes complex questions into sub-questions using the model‚Äôs\ninternal knowledge, then refines preliminary answers via retrieved documents for evidence grounding. It\nfurther introducesInstructional Grounding Distillation (IGD), which distills grounding trajectories from\nChatGPT into smaller open models such as Mistral-7B.\nIn-Context RAG.Selects the top- k retrieved documents using the BGE Reranker and feeds them as context to\nthe LLM for direct answer generation.\nReComp. (Xu et al., 2024)ReComp retrieves relevant documents and compresses them into concise, query-\nrelated summaries via either an extractive or a generative compressor. These summaries are then used as\ncontext for answer generation. Training jointly optimizes both retriever and compressor, allowing selective\nretrieval when documents are unhelpful.\nDPA-RAG. (Dong et al., 2025)This method introduces preference-aligned retrieval and generation. It first\nconstructs preference data by analyzing LLM responses under various retrievals and then aligns both reranker\nand generator through a hybrid of point-wise, pair-wise, and contrastive training objectives.\nRetRobust. (Yoran et al., 2024)Improves robustness of RAG systems through two mechanisms: (i) using an\nNLI model to filter irrelevant retrieved texts, and (ii) fine-tuning with mixed relevant/irrelevant retrieval\nsamples so that the model learns when to utilize or ignore retrieval information.\nChatQA. (Liu et al., 2025)A context-augmented instruction-tuned model that integrates multi-source conversa-\ntional and instruction data to enhance reasoning and refusal capabilities. It also fine-tunes a dense retriever\non multi-turn QA data, replacing traditional query rewriting modules.\nSelf-RAG. (Asai et al., 2024)Incorporatesreflection tokens(e.g., ‚Äúneed retrieval?‚Äù, ‚Äúretrieved relevant?‚Äù, ‚Äúsup-\nported answer?‚Äù) so the model can self-assess and adaptively decide when to retrieve external knowledge.\nTraining combines GPT-4‚Äìgenerated annotated data with self-reflective labeling to enable dynamic retrieval\nand self-critique during inference.\nRAG-DDR. (Li et al., 2025)EmploysDifferentiable Data Rewards (DDR)to achieve fully end-to-end optimization\nof RAG systems. It uses rollout-based system rewards and aligns retrieval and generation through Direct\nPreference Optimization (DPO).\n24\nDRO. (Shi et al., 2025)Models document ordering as a latent variable and alternates between inference and\noptimization using a variational EM framework. The E-step estimates document order distributions via\nimportance sampling, while the M-step jointly updates the selector and generator based on weighted likelihood\nmaximization.\nC Pretraining Data Quality\nTo ensure the quality of the constructed pretraining data, we conducted a manual evaluation. We randomly\nsampled 200 examples for each output type, resulting in a total of 600 samples, which were independently\nassessed by one of the authors. The evaluation results indicate that, thanks to our rigorous filtering process,\nalmost all generated samples successfully cover the key information contained in the source documents. Only\n21 instances exhibited mild hallucinations, where the model introduced information not present in the original\ntext. This demonstrates that the synthesized data are of high factual and semantic quality, providing a\nreliable foundation for compression pretraining.\nModels Corpus size NQ HotpotQA Musique 2Wikiqa Average\nNormal\nMistral-7B\n0.5M 53.38 41.40 10.30 46.67 37.94\n1M 54.82 43.71 10.63 46.90 39.02\n2M 54.64 43.52 10.55 46.58 38.82\nPhi4-mini\n0.5M 48.82 38.53 7.78 43.57 34.67\n1M 48.40 38.47 7.73 43.82 34.61\n2M 49.30 38.62 7.70 43.71 34.83\nOracle\nMistral-7B\n0.5M 70.33 62.47 29.16 57.97 54.98\n1M 74.08 68.88 38.97 63.91 61.46\n2M 73.77 69.51 38.31 64.54 61.53\nPhi4-mini\n0.5M 68.31 64.41 29.25 58.22 55.05\n1M 69.41 64.42 31.32 58.00 55.79\n2M 69.90 65.32 31.77 58.52 56.38\nTable 12Instruction tuning performance ofMistral-7BandPhi4-minimodels under different pretraining corpus\nsizes (0.5M, 1M, 2M). Results are reported on four QA datasets under bothNormalandOracleretrieval settings with\na fixed compression ratio (CR= 32).\nD Pretraining Data Scaling\nTo investigate how the number of pretraining samples affects the performance of the compressor, we train\nmodels with varying amounts of pretraining data and assess their performance after instruction tuning on\nfour QA datasets. The results are illustrated in Table 12. We observe that enlarging the pretraining corpus\ngenerally leads to consistent performance improvements across all datasets and both retrieval settings. For\ninstance, under theNormalsetting, the Mistral-7B model improves its average score from 37.94 to 39.02 as\nthe corpus size increases from 0.5M to 1M, while the performance remains stable when further scaled to 2M.\nA similar trend can be observed in theOraclesetting, where the model achieves an average gain of over 6\npoints when moving from 0.5M to 1M, indicating that additional pretraining data enhances the compressor‚Äôs\nability to preserve more task-relevant information.\nFor the smaller Phi4-mini model, the improvements are relatively modest, suggesting that model capacity\nmay constrain the benefits of scaling pretraining data. Overall, these findings demonstrate that moderate\nexpansion of pretraining data contributes positively to downstream QA performance, while extremely large\npretraining sets bring diminishing returns.\nE Training Curves\nFigures5presentthevalidationlosscurvesduringthecompressionpretrainingstageacrossdifferentcompression\nratios.\nA clear trend emerges: as the compression ratio increases, the validation loss rises for both models. This\neffect is more pronounced forPhi4-mini, where losses at ratios of 128 and 256 diverge sharply. In contrast,\n25\nFigure 5Validation loss curves during the compression pretraining stage under different compression ratios (CR) on the\nPhi-4-mini(left) andMistral-7B(right) models.\nFigure 6Validation trends of recall and evaluation loss during the end-to-end training stage under different compression\nratios (CR) on theNQ(top) andMusique(bottom) datasets.\nMistral-7B exhibits relatively uniform loss gaps across compression ratios. We hypothesize that this difference\narises because of capacity.Phi4-mini, with fewer parameters, has limited representational ability. At very\nhigh compression levels (e.g., CR=128), excessive information loss leads to semantic degradation and a steep\nrise in validation loss.\nFigure 6 presents the validation curves during end-to-end training on theNQandMusiquedatasets. Recall\nscores consistently increase while evaluation losses steadily decrease, indicating stable and effective optimization.\nHigher compression ratios generally yield lower recall and higher loss, mirroring the trends observed during\nthe compression pretraining stage.\nF More Analysis\nF.1 Effect of Freezing the Compressor and Query Reasoner\nWe investigate the effect of limiting the fine-tuning scope to the generator module while freezing both the\ncompressor and query reasoner. Specifically, we examine two representative compression settings,CR=32and\nCR=128, and compare model performance when only the generator is fine-tuned during both the instruction\ntuning and end-to-end QA training stages. The results are shown in Table 13 and 14. During the compression\nlearning and instruction tuning stages, we observe that fine-tuning the compressor alongside the generator\nbrings only marginal improvements. For example, under theNormalsetting, the average gain of full fine-\ntuning over generator-only tuning is less than 2.0% across most datasets. Considering that in Section 4.4,\nthe instruction-tuned compressor tends to degrade retrieval performance due to its focus on answer-centric\nrepresentations, a promising future direction is to explore how to effectively extract task-relevant information\nfrom compressed representations without directly fine-tuning the compressor itself. In contrast, during the\n26\nend-to-end learning stage, fine-tuning the query reasoner proves to be more beneficial. A trainable retrieval\nmodule enables the model to identify more relevant documents and provide stronger contextual grounding\nfor the generator. For instance, under theOraclesetting with a compression ratio of 32, the F1 score of\nMistral-7Bimproves from 52.54% to 70.91% when jointly fine-tuning both the query reasoner and generator.\nThis highlights the crucial role of query reasoner in enhancing overall QA performance within our unified\ntraining framework.\nModels CR NQ HotpotQA Musique 2Wiki Average\nNormal\nGenerator-only\nMistral 32x 52.26 42.66 10.43 45.79 37.78\nMistral 128x 50.72 40.10 9.14 45.52 36.37\nPhi 32x 45.91 37.70 6.95 42.97 33.39\nPhi 128x 38.83 32.30 6.50 42.53 30.04\nFull finetune\nMistral 32x 54.64 43.52 10.55 46.58 38.82\nMistral 128x 53.36 41.37 10.26 46.40 37.85\nPhi 32x 49.30 38.62 7.70 43.71 34.83\nPhi 128x 43.09 33.92 6.87 43.70 31.90\nOracle\nGenerator-only\nmistral 32x 72.78 67.48 34.38 60.89 58.88\nmistral 128x 66.93 59.66 25.94 58.19 52.68\nphi 32x 65.65 62.76 27.60 56.46 53.12\nphi 128x 52.87 47.51 17.38 48.98 41.68\nFull finetune\nmistral 32x 73.77 69.51 38.31 64.54 61.53\nmistral 128x 69.96 62.09 30.86 59.08 55.50\nphi 32x 69.90 65.32 31.77 58.52 56.38\nphi 128x 60.44 51.52 19.28 50.29 45.38\nTable 13Instruction tuning results ofMistralandPhimodels under different fine-tuning scopes (generator-only vs.\nfinetune-both), retrieval modes (Normal vs. Oracle), and compression ratios (CR = 32, 128) on four QA datasets.\nModels CR HotpotQA 2Wiki\nF1 EM F1 EM\nNormal\nGenerator-only\nMistral 32x 38.40 28.24 39.93 35.80\nMistral 128x 38.26 28.26 41.34 37.29\nPhi4 32x 32.91 23.51 35.99 32.14\nPhi4 128x 31.42 21.89 35.54 31.32\nFull finetune\nMistral 32x 41.84 31.26 43.23 38.98\nMistral 128x 42.26 31.78 41.80 37.37\nPhi4 32x 37.14 26.99 38.15 33.82\nPhi4 128x 34.73 24.95 36.41 32.23\nOracle\nGenerator-only\nMistral 32x 52.54 40.11 45.43 40.98\nMistral 128x 51.60 39.06 44.64 40.22\nPhi4 32x 45.91 34.20 40.33 36.12\nPhi4 128x 39.46 28.06 37.05 32.57\nFull finetune\nMistral 32x 70.91 57.07 66.32 61.12\nMistral 128x 66.51 52.30 64.82 58.97\nPhi4 32x 51.06 38.33 50.68 45.41\nPhi4 128x 52.68 38.49 49.97 44.11\nTable 14End-to-end QA performance ofMistralandPhimodels under different fine-tuning scopes (generator-only\nvs. finetune-both), retrieval modes (Normal vs. Gold), and compression ratios (CR = 32, 128) onHotpotQAand\n2WikiQAdatasets.\nF.2 Retrieval number generalization\nWe further explore the impact of varying the number of retrieved documents (top-k) during testing in our\nend-to-end training framework. During training, the model is consistently trained with the top-5 retrieved\ndocuments, while at test time, we varyk from 1 to 10 to examine the model‚Äôs sensitivity to retrieval size.\nThe results are presented in Figure 7. As shown in the figure, the F1 score generally exhibits a rapid\n27\nFigure 7Performance of varying the number of retrieved documents (k) during testing on different QA datasets.\nincrease followed by a gradual decline ask increases. However, the performance drop remains relatively small,\nindicating that our trained query reasoner and generator demonstrate good generalization with respect to the\nnumber of retrieved documents during inference.\nF.3 Effect of Query Reasoner Initialization\nWe evaluate the effect of initializing the query reasoner with the pretrained compressor parameters versus\nrandom initialization on theHotpotQAand2Wikidatasets, as shown in Table 15. The results demonstrate\nthat compressor-initialized models consistently outperform their randomly initialized counterparts across all\nsettings. This performance gain (e.g., from 66.84%‚Üí70.91% F1 and 62.68%‚Üí66.32% F1 on HotpotQA and\n2Wiki, respectively) indicates that the pretrained compressor provides a strong prior for learning effective\nquery reasoning representations, as it already encodes semantic relationships between queries and document\ncontent during the compression pretraining stage.\nModel CR Retrieval Mode HotpotQA 2Wiki\nF1 EM F1 EM\nMistral-7B 32x Normal 39.48 29.12 39.90 35.79\nw/ Compressor Init. 32x Normal 41.84 31.26 43.23 38.98\nMistral-7B 32x Oracle 66.84 52.91 62.68 57.55\nw/ Compressor Init. 32x Oracle 70.91 57.07 66.32 61.12\nMistral-7B 128x Normal 37.25 27.38 38.55 34.69\nw/ Compressor Init. 128x Normal 42.26 31.78 41.80 37.37\nMistral-7B 128x Oracle 62.06 48.37 60.63 54.87\nw/ Compressor Init. 128x Oracle 66.51 52.30 64.82 58.97\nTable 15End-to-End QA Performance with Randomly Initialized vs. Compressor-Initialized Query Reasoner\nF.4 Efficiency Analysis\nWe evaluate the inference efficiency of our framework under different compression ratios. Specifically, for\neach query, we retrieve 20 candidate documents, compress them into 5 document representations using the\ncompressor, and then generate the final answer based on these 5 compressed representations and the query.\nThe average inference time for each stage is reported in Table 16. All timing statistics are measured on a\nsingle NVIDIA H100 GPU with 80GB memory.\nAs shown in the results, decoding with compressed representations takes only about 40% of the time required\nwhen using full-text documents. Although compressing 20 documents is relatively time-consuming, this step\ncan be performed offline; hence, it does not affect real-time inference latency during query answering. This\nmakes the overall computational cost acceptable for practical deployment. We also observe that for the\nMistralmodel, compression time tends to decrease as the compression ratio increases, while both decoding\nand query retrieval times remain relatively stable across different compression settings.\n28\nModels CR Compression Time Query Time Decoding Time\nMistral-7B Pure text ‚Äì ‚Äì 1290.57\nMistral-7B 4x 1092.29 99.69 532.73\nMistral-7B 16x 922.85 94.17 502.78\nMistral-7B 32x 904.22 92.16 514.75\nMistral-7B 64x 893.76 95.14 521.09\nMistral-7B 128x 876.99 95.24 518.41\nMistral-7B 256x 835.87 90.76 521.03\nPhi4-mini Pure text ‚Äì ‚Äì 870.29\nPhi4-mini 4x 674.78 94.34 342.05\nPhi4-mini 16x 574.46 89.53 343.01\nPhi4-mini 32x 561.17 84.35 358.04\nPhi4-mini 64x 604.89 85.33 354.77\nPhi4-mini 128x 594.55 91.73 360.23\nPhi4-mini 256x 789.49 99.47 354.87\nTable 16Average inference time (in milliseconds) for compression, retrieval, and decoding across different compression\nratios (CR) onMistral-7BandPhi4-minimodels.\nG Fidelity and Grounding Analysis\nIn this section, we aim to understand how much essential information is retained in our compressed represen-\ntations, and to what extent the generated answers remain grounded to the input documents and queries after\nboth compression learning and end-to-end training.\nG.1 Information Preservation\nDuring compression representation pretraining, we include a paraphrasing objective that allows the generation\nmodel to reconstruct the original text from the compressed representation. We consider two evaluation\nsettings: (1)unseen data, consisting of positive documents of downstream QA tasks that were not used in\npretraining, and (2)seen data, where we randomly sample 4,000 documents from the pretraining corpus.\nWe evaluate the reconstruction quality using several metrics:BERTScore(Zhang* et al., 2020) (which\nmeasures semantic similarity between texts),ROUGE-1andROUGE-L(which capture lexical overlap), and\nfollowing ≈Åajewska et al. (2025), we also compute theentity preservation ratio, which measures the proportion\nof entities from the input text that are preserved in the reconstructed text4.\nThe results are shown in Table 17. We observe that our model achieves a high BERTScore of nearly 90%,\nwhich remains stable across different compression ratios. This indicates that the compressed representations\nsuccessfully retain most of the semantic information from the original text. For ROUGE-1, ROUGE-L, and\nentity preservation, the model also maintains relatively high scores‚Äîover 50% on average. We further observe\nthat as the compression ratio increases, the lexical overlap and entity preservation metrics gradually decline,\nsuggesting that fewer memory tokens make it harder to reconstruct the exact surface form of the original\ntext. However, the consistently high semantic similarity scores imply that the key meaning is preserved. This\nphenomenon may indicate that when using fewer memory tokens, the model tends to generate paraphrased\nexpressions to maintain the original semantics. We leave further exploration of this linguistic compression\nbehavior for future work.\n4Entity extraction is performed using the SpaCy library.\n29\nModels CR Seen Data Unseen Data\nBERT R-1 R-L Entity BERT R-1 R-L Entity\nMistral-7B\n4x 90.67 55.88 40.12 54.78 91.45 59.74 44.09 60.04\n16x 90.63 56.12 40.33 54.78 91.43 59.97 44.10 59.88\n32x 90.56 56.21 40.10 53.91 91.39 60.28 43.98 59.33\n64x 90.28 55.54 38.86 51.45 91.24 60.09 43.42 58.26\n128x 89.84 54.12 36.56 47.75 91.00 59.61 42.48 55.75\n256x 89.19 51.75 33.12 42.12 90.51 57.89 39.59 52.38\nPhi4-mini\n4x 90.93 58.48 42.16 57.86 91.70 62.00 45.10 63.14\n16x 90.77 58.20 41.49 56.28 91.66 62.20 45.31 62.22\n32x 90.36 57.04 39.40 52.38 91.42 61.71 44.34 59.64\n64x 89.53 54.27 35.40 45.28 90.84 60.20 41.72 54.47\n128x 88.26 49.30 29.65 34.98 89.61 55.68 35.58 43.89\n256x 88.13 49.05 29.10 34.27 89.52 55.27 35.22 43.61\nTable 17Evaluation of information preservation under different compression ratios (CR) on seen and unseen documents\nusingBERTScore,ROUGE, and entity preservation.\nModels CR Retrieval NQ HotpotQA Musique 2Wiki Average\nFaith Fc Faith Fc Faith Fc Faith Fc Faith Fc\nPretraining-initialized\nMistral-7B\n4x\nNormal\n81.57 8.39 67.42 11.80 55.80 9.57 56.45 5.15 65.31 8.73\n16x 75.85 8.97 62.50 13.58 49.64 10.34 51.02 5.04 59.75 9.48\n32x 72.65 7.34 61.40 11.86 51.35 8.18 53.30 7.30 59.67 8.67\n64x 67.49 8.29 57.95 10.85 46.55 10.00 44.44 5.16 54.11 8.57\n128x 65.99 8.67 56.50 10.35 44.52 9.50 43.26 4.54 52.57 8.27\n256x 64.74 7.42 53.68 12.34 40.79 7.87 40.88 4.85 50.02 8.12\n4x\nOracle\n86.73 9.42 83.75 18.72 67.67 13.60 80.16 6.26 79.58 12.00\n16x 81.13 9.74 83.16 19.19 63.89 11.05 73.96 4.12 75.54 11.03\n32x 79.34 8.16 81.87 16.31 65.27 10.79 71.60 3.71 74.52 9.74\n64x 74.43 8.07 78.39 16.63 55.89 10.09 67.61 4.30 69.08 9.77\n128x 74.63 10.40 77.17 12.60 55.14 10.47 61.62 4.00 67.14 9.37\n256x 71.13 9.63 73.69 16.49 51.35 10.22 56.03 4.67 63.05 10.25\nPhi4-mini\n4x\nNormal\n79.45 7.47 62.46 10.06 50.57 11.13 51.05 5.27 54.69 8.48\n16x 74.99 8.13 61.45 12.84 51.61 10.04 48.59 5.17 59.16 9.05\n32x 66.58 5.74 59.22 11.64 45.44 9.03 45.16 3.69 54.10 7.52\n64x 59.66 8.36 50.44 9.42 39.52 8.53 39.79 4.13 47.35 7.61\n128x 60.26 8.18 51.83 10.57 35.08 9.47 34.72 4.61 45.48 8.20\n256x 57.65 7.78 45.57 8.71 33.79 7.38 32.73 5.71 42.44 7.39\n4x\nOracle\n82.51 8.32 82.88 18.79 60.67 11.02 76.81 4.19 75.72 10.58\n16x 81.00 8.81 81.73 17.55 63.38 10.54 69.61 3.03 73.93 9.98\n32x 75.42 10.20 74.79 16.72 54.49 10.30 61.67 3.26 66.59 10.12\n64x 68.72 7.66 71.63 15.60 49.55 7.51 56.36 2.63 61.56 8.35\n128x 63.49 8.99 66.46 14.84 44.17 8.38 48.50 2.07 55.65 8.57\n256x 61.36 9.80 64.48 11.60 44.01 10.76 53.23 3.10 55.77 8.82\nInstruction-tuned-initialized\nMistral-7B\n4x\nNormal\n54.67 39.69 30.21 36.15 12.68 11.08 5.50 38.97 25.76 31.47\n16x 52.13 39.13 37.68 39.91 13.12 12.55 11.13 36.17 28.52 31.94\n32x 49.33 37.64 36.08 36.97 12.25 10.29 12.93 40.87 27.65 31.44\n64x 50.60 36.92 35.35 36.71 11.84 10.52 13.50 34.24 27.82 29.60\n128x 49.03 36.40 36.52 39.58 11.77 10.56 12.13 35.57 27.36 30.53\n256x 50.90 40.02 34.95 38.75 11.81 11.41 15.73 37.67 28.35 31.96\n4x\nOracle\n69.55 71.58 67.94 68.23 23.86 37.92 38.43 64.61 49.95 60.59\n16x 69.57 67.79 60.25 60.83 18.85 27.60 39.90 60.19 47.14 54.10\n32x 65.97 63.60 59.77 61.72 21.91 30.38 33.40 56.57 45.26 53.07\n64x 63.90 62.81 57.47 56.46 18.60 21.99 27.40 52.27 41.84 48.38\n128x 68.57 60.98 55.75 57.86 16.19 21.43 29.40 55.38 42.48 48.91\n256x 65.60 63.81 55.46 56.30 16.80 20.74 28.40 52.97 41.57 48.46\nPhi4-mini\n4x\nNormal\n49.50 41.02 28.13 35.57 12.65 13.56 6.37 34.37 24.16 31.13\n16x 41.57 30.75 27.70 32.59 9.82 8.21 7.87 35.37 21.74 26.73\n32x 41.07 26.85 28.64 30.10 8.58 8.97 10.63 31.36 22.23 24.32\n64x 35.98 25.25 28.57 29.71 11.37 11.03 10.13 34.87 21.51 25.21\n128x 38.47 26.75 30.23 29.52 8.06 8.25 11.03 31.00 21.95 23.88\n256x 37.50 27.89 26.48 25.10 9.34 9.03 10.80 31.74 21.03 23.44\n4x\nOracle\n65.13 59.84 43.67 52.32 14.57 22.90 18.37 48.38 35.43 45.86\n16x 66.45 62.90 46.57 52.31 13.28 18.58 21.87 47.04 37.04 45.21\n32x 60.20 53.13 46.27 45.89 13.70 17.42 25.30 43.71 36.37 40.04\n64x 57.35 52.66 47.40 48.79 13.18 18.28 18.63 43.99 41.13 40.93\n128x 55.30 51.05 42.34 42.43 12.80 17.51 18.67 44.84 32.28 38.96\n256x 52.80 47.28 45.35 46.32 11.98 14.93 23.50 40.64 33.41 37.29\nTable 18Grounding evaluation ofMistral-7BandPhi4-minimodels under different initialization settings (pretraining-\ninitialized vs. instruction-tuned-initialized), retrieval modes (Normal vs. Oracle), and compression ratios (CR). Metrics\nincludeFaithfulness (Faith)andFactual Correctness (Fc)across four QA datasets.\n30\nG.2 Grounding Analysis\nWe further evaluate the grounding quality between the generated answers and the compressed document\nrepresentations under both compression evaluation and end-to-end evaluation settings. We adopt the\nRAGAs(Es et al., 2025) package, which implements theLLM-as-a-Judgeparadigm for assessing generation\nquality. Two key metrics are used:faithfulness, which measures whether the generated answer is faithful to\nthe provided context and relevant to the query, andfactual correctness, which evaluates whether the answer is\nfactually supported by the context. We employ GPT-4o-mini as the judging model.\nThe results are presented in Table 18. For the compression evaluation, our model achieves consistently high\nfaithfulness scores, particularly when positive documents are included, indicating that the model generates\nanswers more closely aligned with the query. However, the factual correctness scores are comparatively\nlower, consistent with findings reported in ≈Åajewska et al. (2025). We hypothesize that this is because, after\ninstruction tuning, the generation model tends to produce longer and more elaborative answers, occasionally\nintroducing tokens that do not appear in the original context. We also observe a decreasing trend in faithfulness\nas the compression ratio increases and model size decreases.\nFor the end-to-end evaluation, the model demonstrates strong performance across both metrics. In particular,\nunder the Mistral-7B model with a compression ratio of 4 and the top-20 retrieval setting containing positive\ndocuments, faithfulness and factual correctness reach 49.95 and 60.59, respectively. The higher factual\ncorrectness is likely due to the use of short gold answers during training, which encourages the model to\ndirectly copy relevant words or phrases from the retrieved documents. Other observed trends are consistent\nwith those in the compression evaluation results.\nH Paraphrase Case Study\nTo better understand the nature of information captured by our compressed representations and the query\nreasoner, we analyze how effectively the compressed representations preserve key semantic information. As\nillustrated in Table 19, the generations conditioned on compressed representations largely preserve the essential\nsemantic content of the original documents, while substantially altering surface expressions such as sentence\nstructure and word order. This observation is consistent with our pretraining objective, which encourages\nthe model to encode core semantic meaning rather than memorize lexical sequences. The model‚Äôs ability to\nreconstruct paraphrased variants of the original text demonstrates that the learned compressed representations\nsuccessfully capture high-level semantic knowledge in a compact form.\n(a)Without MSE\n (b)With MSE\nFigure 8t-SNE visualization of document representations and compressed (memory token) representations ofMistral-\n7Bunder the compression ratio of 32.\n31\nExamples of Original and Paraphrased Documents Generated from Compressed Represen-\ntations\nExample 1\nOriginal Document:This a document about SMK Bukit Kepayang. 1) Tuan Haji Mat Rohani Bin Kamar\nwas the first Principal of the School. (2004 - 2008); 2) Madam Hajah Sadariah Binti Haji Md. Jali is the current\nPrincipal of the school. (2008-2014). Madam Hajah Sadariah achieved the title Master Principle in the year 2011\ndue to outstanding performance and achievements in that year.\nParaphrased Document:From 2004 to 2008,Tuan Haji Mat Rohani Bin Kamarserved as the Princi-\npal of SMK Bukit Kepayang. He was succeeded byMadam Hajah Sadariah Binti Haji Md. Jali, who led\nthe school until 2014. In 2011, she was honored with the title ofMaster Principalfor her exceptional achievements.\nExample 2\nOriginal Document:This is a document about Sikorsky SH-60 Seahawk and the U.S. 5th Fleet. The ALMDS\nis a sensor system designed to detect, classify, and localize floating and near-surface moored mines in littoral\nzones, straits, and choke points. The system is operated from an MH-60S, which gives it a countermine role\ntraditionally handled by the MH-53E Sea Dragon, allowing smaller ships the MH-53E can‚Äôt operate from to be\nused in the role. The ALMDS beams a laser into the water to pick up reflections, then produces a video im-\nage for ground personnel to determine if the object is a mine. The MH-60S will utilize the BAE Systems Archerfish.\nParaphrased Document:The ALMDS system, designed to detect and classify floating and near-surface mines\nin strategic locations such as straits, littoral zones, and choke points, is operated from an MH-60S helicopter.\nThis platform enables the U.S. Navy‚Äôs 5th Fleet to perform a countermine mission that was previously the\nresponsibility of the MH-53E Sea Dragon. The MH-60S is equipped with the BAE Systems Archerfish system,\nwhich plays a key role in the process. The ALMDS emits a laser pulse into the water, which reflects.\nExample 3\nOriginal Document:This a document about Pinewild Women‚Äôs Championship. The Pinewild Women‚Äôs\nChampionship was a golf tournament on the LPGA Tour, played only in 1995. It was played at the Pinewild\nCountry Club of Pinehurst in Pinehurst, North Carolina. Rosie Jones was the winner, beating Dottie Pepper on\nthe first hole of a sudden-death playoff.\nParaphrased Document:In 1995, the Pinewild Women‚Äôs Championship took place as a single-year event on\nthe LPGA Tour. The competition was held at the Pinewild Country Club of Pinehurst, located in Pinehurst,\nNorth Carolina. Rosie Jones emerged victorious, securing the title by defeating Dottie Pepper in a sudden-death\nplayoff on the first hole.\nTable 19Examples of Original and Paraphrased Documents generated from compressed representations.\nI Prompts\nFigures 9‚Äì16 illustrate the prompts used during the data synthesis process. Specifically, we employ different\nprompting strategies for (1) generating QA pairs, (2) producing paraphrased documents, (3) validating\ninformation completeness, and (4) completing missing information. Additionally, Figure 17 shows the prompt\ntemplate used by the generation model to answer questions based on the compressed document representations.\n32\nPrompt for Simple Question Generation\nYou are given a document delimited by <doc> and </doc>. Your job is to read the given\ndocument and generate a comprehensive set of multi-hop questions that fully cover all\nthe key information in the text.\n<doc>\n<INSERT DOCUMENT HERE>\n</doc>\nQuestion Requirements:\nYou should generate as many questions as necessary to fully cover all the key facts in\nthe document.\n(1) Each question must be self-contained, meaning it should be understood by the user\nwithout seeing the document.\n(2) Each question must cover only one or at most two distinct key pieces of information.\n(3) The questions must be non-overlapping ‚Äî no two questions should target the same\npiece of information.\n(4) The questions should be simple factual recall only ‚Äî do not require inference,\nreasoning, or summarization.\n(5) Your output should be a list of self-contained, non-overlapping factual questions\nthat together comprehensively cover all the key information in the document.\nThere are some examples:\n{3 demonstrations}\nYour output should be a JSON object with the following format:\n{\n\"Question1\": \"...\",\n\"Question2\": \"...\",\n...,\n\"QuestionN\": \"...\"\n}\nFigure 9Prompt used for simple question generation.\n33\nModels Data composition NQ HotpotQA Musique 2Wikiqa Average\nNormal\nMistral-7B\nNo-pretrain 53.03 40.63 9.68 46.64 37.50\nSimpleQA 53.84+0.8142.20+1.5710.26+0.5846.68+0.0438.25+0.75\nPara 54.52+1.4943.05+2.4210.51+0.8346.41-0.2338.62+1.12\nSimpleQA+ComplexQA 55.48+2.4543.00+2.3710.67+0.9946.39-0.2538.88+1.38\nSimpleQA+ComplexQA+Para 54.64+1.6143.52+2.8910.55+0.8746.58-0.0638.82+1.32\nPhi4-mini\nNo-pretrain 48.10 37.65 7.61 44.68 34.51\nSimpleQA 48.56+0.4638.91+1.268.19+0.5843.70-0.9834.84+0.33\nPara 48.65+0.5538.41+0.767.74+0.1344.11-0.5734.73+0.22\nSimpleQA+ComplexQA 49.47+1.3738.88+1.238.03+0.4243.96-0.7235.08+0.57\nSimpleQA+ComplexQA+Para 49.30+1.2038.62+0.977.70+0.0943.71-0.9734.83+0.32\nTable 20Effect of pretraining data composition on instruction-tuning performance under Normal (top-5 retrieval)\nsettings under the 32 compression ratio. We report the absolute score change (¬±) for each pretraining data setting\nrelative to the No-pretrain baseline.\nModels CR NQ HotpotQA Musique 2Wikiqa Average\nNormal\nMistral-7B 32x 54.25 43.11 9.85 45.84 38.26\nw/ mse 32x 54.64+0.3943.52+0.4110.55+0.7046.58+0.7438.82+0.56\nMistral-7B 128x 52.98 41.32 10.22 46.23 37.69\nw/ mse 128x 53.36+0.3841.37+0.0510.26+0.0446.40+0.1737.85+0.16\nTable 21Instruction-tuning performance with and without the MSE loss under different compression ratios (CR = 32,\n128) and normal retrieval settings.\n34\nPrompt for Complex Question Generation\nYou are given a document delimited by <doc> and </doc>. Your job is to generate a set\nof MULTI-HOP questions that, taken together, comprehensively cover the document‚Äôs key\ninformation.\n<doc>\n<INSERT DOCUMENT HERE>\n</doc>\nQuestion Requirements:\n1) Self-contained: Every question must be understandable without viewing the document.\n2) Multi-hop only: Each question must require at least TWO independent\npieces of evidence from DIFFERENT parts of the document (e.g., different\nparagraphs/sections/tables/items). If a question can be answered from a single sentence\nor data point, REJECT it.\n3) Non-overlapping: No two questions may target the same fact or the same combination of\nfacts. Each question must have a unique reasoning path and evidence combination.\n4) Coverage: Produce as many questions as needed to cover ALL key facts in the document.\nPrefer many small, precise multi-hop questions over a few large ones.\n5) Focus: Each question should target ONE multi-hop objective, typically integrating\n2‚Äì3 facts (bridging, comparison, aggregation, temporal/causal linking, entity‚Äìattribute\njoining, etc.). Do NOT bundle multiple unrelated sub-questions.\n6) Verifiability: The answer to each question must be derivable SOLELY from the document,\nwith no external knowledge or subjective judgment.\n7) Clarity: Avoid yes/no questions and vague wording. Use explicit constraints,\nquantities, and identifiers where relevant.\n8) No explanations: Do NOT include rationales, steps, or references‚ÄîONLY output the\nquestions as JSON.\n9) You can generate 2-hops, 3-hops, 4-hops, etc. questions.\nQUESTION TEMPLATES (use as patterns, adapt as needed)\n- Bridging: \"Which X satisfies BOTH condition A mentioned in [context A] AND condition B\nmentioned in [context B]?\"\n- Comparison: \"Considering [pivot], which of X or Y meets [criterion] when combining\ndetails from [source 1] and [source 2]?\"\n- Aggregation: \"When combining [quantity/info] from section A with [quantity/info] from\nsection B, which single entity matches [combined constraint]?\"\n- Temporal/Causal: \"Based on the timeline described in parts A and B, which event/entity\nfulfills [temporal/causal relation]?\"\nThere are some examples: {3 demonstrations}\nInput FORMAT:\nDocument:\n<INSERT DOCUMENT HERE>\nOUTPUT FORMAT\nReturn ONLY a JSON object with keys \"Question1\", \"Question2\", ..., \"QuestionN\".\nExample (structure only):\n{ \"Question1\": \"...\",\n\"Question2\": \"...\",\n\"Question3\": \"...\",\n\"QuestionN\": \"...\" }\nFigure 10Prompt used for complex question generation.\n35\nPrompt for Answer Generation\nYou are a factual answering assistant.\nYour task is to read the provided document and answer the given question **based only\non the information explicitly stated in the document**.\nPlease output the answer as short as possible.\nRequirements:\n- Your answer must be based solely on the content of the document.\n- Do not use prior knowledge or make assumptions beyond the document.\n- If the document does not contain the answer, respond with: \"The document does not\ncontain this information.\"\n- The answer should be concise, factual, and complete.\nInput Format:\nDocument:\n<INSERT DOCUMENT TEXT HERE>\nQuestion:\n<INSERT QUESTION HERE>\nOutput Format:\nAnswer: <YOUR ANSWER HERE>\nFigure 11Prompt used for factual answer generation.\n36\nPrompt for QA Validation\nYou are a fact-checking assistant.\nYour task is to verify whether the given answer to a question is **fully supported by\nthe provided document**.\nInstructions:\n- Read the document carefully.\n- Read the question and the provided answer.\n- Determine whether the answer is correct **based solely on the information in the\ndocument**.\n- The answer must be **complete**, **factually correct**, and **not contain any\ninformation that is not in the document**.\nIf the answer is fully correct and supported by the document, respond with:\n\"Correct\"\nIf the answer is partially correct, incomplete, or includes unsupported information,\nrespond with:\n\"Incorrect\"\nInput Format:\nDocument:\n<INSERT DOCUMENT HERE>\nQuestion:\n<INSERT QUESTION HERE>\nAnswer:\n<INSERT ANSWER HERE>\nOutput Format:\n{{\"Judgment\": \"Correct\" / \"Incorrect\"}}\nFigure 12Prompt used for QA validation.\n37\nPrompt for Supplementary Simple QA Generation\nYou are given a document and a set of existing question-answer pairs. Your task is to\ncarefully compare the information covered in the QA pairs against the document and\ngenerate additional questions that cover any key information not yet addressed.\nRequirements:\n- Only generate questions for key facts present in the document that are **not already\ncovered** in the existing QA pairs.\n- Do **not** repeat or rephrase the information in the existing question answer pairs.\n- Each question should cover **only one or two distinct key pieces of information**.\n- Each question must be self-contained, meaning it should be understood by the user\nwithout seeing the document.\n- All questions should require **simple factual recall only**, with no inference or\nreasoning.\nThere are some examples:\n{3 demonstrations}\nInput Format:\nDocument:\n<INSERT DOCUMENT HERE>\nExisting QA:\n<INSERT EXISTING QA HERE>\nOutput Format:\nReturn your generated new supplementary questions in the following JSON format:\n{\n\"Number of Supplementary Questions\": N,\n\"Question1\": \"...\",\n\"Question2\": \"...\",\n...,\n\"QuestionN\": \"...\"\n}\nIf all key information is already covered and no supplementary questions are needed,\noutput an empty JSON object:\n{\n\"Number of Supplementary Questions\": 0\n}\nFigure 13Prompt used for supplementary Simple QA generation.\n38\nPrompt for Supplementary Complex QA Generation\nYou are given a document and a set of existing question-answer pairs. Your task is to\ncarefully compare the information covered in the QA pairs against the document and generate\nadditional MULTI-HOP questions that cover any key information not yet addressed.\nRequirements:\n- Only generate questions for key facts present in the document that are **not already\ncovered** in the existing question answer pairs.\n- Do **not** repeat or rephrase information which can be found in the existing question answer\npairs.\nQuestion Requirements:\n1) Self-contained: Every question must be understandable without viewing the document.\n2) Multi-hop only: Each question must require at least TWO independent pieces of evidence\nfrom DIFFERENT parts of the document (e.g., different paragraphs/sections/tables/items). If a\nquestion can be answered from a single sentence or data point, REJECT it.\n3) Non-overlapping: No two questions may target the same fact or the same combination of\nfacts. Each question must have a unique reasoning path and evidence combination.\n4) Coverage: Produce as many questions as needed to cover ALL key facts in the document.\nPrefer many small, precise multi-hop questions over a few large ones.\n5) Focus: Each question should target ONE multi-hop objective, typically integrating 2‚Äì3\nfacts (bridging, comparison, aggregation, temporal/causal linking, entity‚Äìattribute joining,\netc.). Do NOT bundle multiple unrelated sub-questions.\n6) Verifiability: The answer to each question must be derivable SOLELY from the document,\nwith no external knowledge or subjective judgment.\n7) Clarity: Avoid yes/no questions and vague wording. Use explicit constraints, quantities,\nand identifiers where relevant.\n8) No explanations: Do NOT include rationales, steps, or references‚ÄîONLY output the questions\nas JSON.\n9) You can generate 2-hops, 3-hops, 4-hops, etc. questions.\nQUESTION TEMPLATES (use as patterns, adapt as needed)\n- Bridging: \"Which X satisfies BOTH condition A mentioned in [context A] AND condition B\nmentioned in [context B]?\"\n- Comparison: \"Considering [pivot], which of X or Y meets [criterion] when combining details\nfrom [source 1] and [source 2]?\"\n- Aggregation: \"When combining [quantity/info] from section A with [quantity/info] from\nsection B, which single entity matches [combined constraint]?\"\n- Temporal/Causal: \"Based on the timeline described in parts A and B, which event/entity\nfulfills [temporal/causal relation]?\"\nInput Format:\nDocument: <INSERT DOCUMENT HERE> Existing QA: <INSERT EXISTING QA HERE>\nOutput Format:\nNote, do not repeat or paraphrase existing questions. Instead, generate new multi-hop\nquestions for the missing information, and put the new questions in JSON format:\n{\n\"Number of Supplementary Questions\": N,\n\"Question1\": \"...\",\n...,\n\"QuestionN\": \"...\"\n}\nIf all key information is already covered and no supplementary questions are needed, output\nan empty JSON object:\n{\n\"Number of Supplementary Questions\": 0\n}\nFigure 14Prompt used for supplementary complex QA generation.\n39\nPrompt for Document Paraphrasing\nYou are given a document. Your task is to paraphrase the document in a way that:\n(1) **Restructure extensively** ‚Äî **Try your best to break down the structure of\nthe original document**, do not keep the same paragraphing, ordering, or sentence\nflow. Reorganize ideas, shuffle the order, merge or split sentences, and restructure\narguments.\n(2) **Preserve meaning with absolute accuracy** ‚Äî ensure that ALL key information\nand semantics of the original document are retained. Do not omit any factual details,\nnumbers, dates, or specific information.\n(3) **Avoid direct copying** ‚Äî no sentence should remain identical; re-express ideas in\na fresh way using synonyms and varied sentence structures.\n(4) **CRITICAL: Add no new information** ‚Äî the paraphrased document cannot introduce:\n- New facts, interpretations, or context not explicitly stated\n- Organizational names or affiliations not mentioned in the original\n- Explanatory details or background information\n- Your own analysis or conclusions about the content\n(5) **Maintain the original‚Äôs voice and perspective** ‚Äî if the original uses commands\n(\"followers shall...\"), don‚Äôt change it to descriptions (\"the organization promotes...\").\nPreserve the document‚Äôs intended tone and format.\n(6) **Verify factual relationships** ‚Äî when paraphrasing complex information involving\nmultiple entities, dates, or cause-and-effect relationships, double-check that you\nmaintain the correct connections and chronology.\n(7) **Use varied vocabulary** ‚Äî employ synonyms and alternative expressions while\nmaintaining precision.\n(8) **Preserve completeness** ‚Äî if the original mentions specific numbers, dates, names,\nor measurements, include them in your paraphrase (even if reworded).\n(9) **Maintain coherence** ‚Äî the paraphrased version should read as natural and fluent\nwriting.\n**WARNING: Do not assume context.** If a document mentions \"the Samaj\" without\nidentifying what organization this refers to, do not assume it‚Äôs \"Brahma Samaj\" or any\nother specific group. Work only with what is explicitly stated.\nProduce a paraphrased version that keeps the meaning and all factual details but has\nsignificantly altered structure and wording.\nHere are some examples of paraphrased documents.\n{10 demonstrations}\nInput:\n<document>\nFigure 15Prompt used for document paraphrasing.\n40\nPrompt for Paraphrase Validation\nYou are given two texts:\nOriginal Document ‚Äì the source text containing the key information.\nParaphrased Document ‚Äì a rewritten version of the original.\nYour task is to check whether the paraphrased document fully preserves all the key\ninformation from the original document, without adding any new information.\nGuidelines:\n(1) ‚Äò‚ÄòKey information‚Äô‚Äô means the essential facts, arguments, data, and main ideas of\nthe original.\n(2) The paraphrased document must:\n(2.1) Contain all the key information from the original.\n(2.2) Not omit any important point.\n(2.3) Not introduce information that is absent in the original.\n(2.4) Preserve meanings without distortion or contradiction.\n(2.5) Differences in style, sentence structure, or wording are acceptable as long as the\nmeaning is preserved.\n(3) Output format:\n(3.1) Answer ‚Äò‚ÄòYes‚Äô‚Äô if the paraphrased document completely retains all key information\nand introduces no new information.\n(3.2) Answer ‚Äò‚ÄòNo‚Äô‚Äô if any key information is lost, altered, or if extra/unwarranted\ninformation appears.\nDon‚Äôt output any explanation.\nInput:\nOriginal Document: [insert original doc]\nParaphrased Document: [insert paraphrased doc]\nOutput:\nYes / No\nFigure 16Prompt used for paraphrase validation.\nPrompt for Document-based QA\nYou are a helpful assistant. Your task is to extract relevant information from provided\ndocuments and to answer to questions as briefly as possible.\nBackground: {docs}\nQuestion:{question}\nFigure 17Prompt used for document-based question answering.\n41\n"}
