{"id": 1, "question": "What is CLaRa and what problem does it solve?", "reference_answer": "CLaRa (Compressing LLM-based Retriever Architecture) is a system that compresses retrieved documents into compact memory representations to improve retrieval-augmented generation by reducing context length.", "category": "factual", "keywords": ["compression", "retrieval", "RAG", "memory"]}
{"id": 2, "question": "How many memory tokens does CLaRa use per document?", "reference_answer": "CLaRa uses a configurable number of memory tokens, with experiments showing compression rates like 32x or 64x, typically resulting in 2-4 memory tokens per document.", "category": "technical", "keywords": ["memory tokens", "compression rate", "32x", "64x"]}
{"id": 3, "question": "What are the three training stages of CLaRa?", "reference_answer": "Stage 1: Compression Pretraining with MSE loss, Stage 2: Compression Instruction Tuning, Stage 3: End-to-End Fine-tuning with both compression and generation.", "category": "detailed", "keywords": ["stage 1", "stage 2", "stage 3", "pretraining", "instruction tuning", "end-to-end"]}
{"id": 4, "question": "What loss functions are used in CLaRa Stage 1 training?", "reference_answer": "Stage 1 uses MSE loss for compression alignment between memory tokens and non-memory tokens, and optionally QA loss for semantic preservation.", "category": "technical", "keywords": ["MSE loss", "QA loss", "compression", "alignment"]}
{"id": 5, "question": "How does CLaRa differ from standard RAG systems?", "reference_answer": "CLaRa compresses retrieved documents into compact memory representations instead of directly concatenating full document text, reducing context length while preserving information.", "category": "comparative", "keywords": ["compression", "standard RAG", "memory tokens", "context length"]}
{"id": 6, "question": "What is the compression rate used in CLaRa experiments?", "reference_answer": "CLaRa experiments use compression rates of 32x and 64x, meaning documents are compressed to 1/32 or 1/64 of their original length.", "category": "technical", "keywords": ["compression rate", "32x", "64x"]}
{"id": 7, "question": "What datasets were used to evaluate CLaRa?", "reference_answer": "CLaRa was evaluated on multiple QA datasets including Natural Questions, TriviaQA, HotpotQA, and other multi-hop reasoning benchmarks.", "category": "factual", "keywords": ["Natural Questions", "TriviaQA", "HotpotQA", "datasets"]}
{"id": 8, "question": "What is the role of memory tokens in CLaRa?", "reference_answer": "Memory tokens serve as compressed representations of documents, capturing key information while reducing context length for efficient retrieval-augmented generation.", "category": "detailed", "keywords": ["memory tokens", "compressed representation", "key information"]}
{"id": 9, "question": "What base model does CLaRa build upon?", "reference_answer": "CLaRa is built on top of LLaMA-based models or similar decoder-only transformers, using them as the compression and generation backbone.", "category": "factual", "keywords": ["LLaMA", "decoder", "transformer", "base model"]}
{"id": 10, "question": "How does CLaRa handle multi-hop questions?", "reference_answer": "CLaRa compresses multiple retrieved documents and allows the model to reason across compressed representations to answer multi-hop questions.", "category": "detailed", "keywords": ["multi-hop", "multiple documents", "reasoning"]}
{"id": 11, "question": "What is the purpose of the compressor in CLaRa?", "reference_answer": "The compressor transforms long documents into compact memory token representations while preserving semantic information needed for question answering.", "category": "technical", "keywords": ["compressor", "transformation", "semantic information"]}
{"id": 12, "question": "What attention mechanism does CLaRa use?", "reference_answer": "CLaRa uses standard transformer attention mechanisms, optionally with Flash Attention 2 for efficiency.", "category": "technical", "keywords": ["attention", "transformer", "Flash Attention"]}
{"id": 13, "question": "How is the compression quality measured in CLaRa?", "reference_answer": "Compression quality is measured through MSE loss between memory and non-memory token representations, and through downstream QA performance.", "category": "technical", "keywords": ["MSE loss", "quality", "QA performance"]}
{"id": 14, "question": "What is the doc_max_length parameter in CLaRa?", "reference_answer": "doc_max_length defines the maximum length of input documents before compression, typically set to values like 128 or 256 tokens.", "category": "technical", "keywords": ["doc_max_length", "input length", "128", "256"]}
{"id": 15, "question": "Does CLaRa use LoRA for training?", "reference_answer": "Yes, CLaRa can use LoRA (Low-Rank Adaptation) for efficient fine-tuning of the model, particularly for the decoder and compressor components.", "category": "technical", "keywords": ["LoRA", "fine-tuning", "efficient"]}
{"id": 16, "question": "What optimizer is used for CLaRa training?", "reference_answer": "CLaRa uses AdamW optimizer for training, with learning rates typically around 1e-4 for pretraining and 5e-6 for end-to-end fine-tuning.", "category": "technical", "keywords": ["AdamW", "optimizer", "learning rate"]}
{"id": 17, "question": "How does CLaRa handle document retrieval?", "reference_answer": "CLaRa uses external retrieval systems to fetch relevant documents, then compresses them using trained memory tokens for efficient processing.", "category": "detailed", "keywords": ["retrieval", "external system", "compression"]}
{"id": 18, "question": "What is the advantage of compression in CLaRa?", "reference_answer": "Compression reduces context length, enabling processing of more documents within GPU memory limits while maintaining retrieval performance.", "category": "comparative", "keywords": ["compression advantage", "context length", "GPU memory", "performance"]}
{"id": 19, "question": "What is the training batch size used in CLaRa?", "reference_answer": "CLaRa uses varying batch sizes depending on the stage, with micro batch sizes for gradient accumulation and distributed training across GPUs.", "category": "technical", "keywords": ["batch size", "gradient accumulation", "distributed"]}
{"id": 20, "question": "How does CLaRa preserve semantic information during compression?", "reference_answer": "CLaRa preserves semantics through MSE loss alignment and QA loss, ensuring compressed representations retain information needed for answering questions.", "category": "detailed", "keywords": ["semantic preservation", "MSE loss", "QA loss", "information retention"]}
